\documentclass[a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage[round]{natbib}
\usepackage{subfigure}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{walkr}

\begin{document}

%% do not edit, for illustration only
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{YY}
\year{20ZZ}
\month{AAAA}

\begin{article}

\title{walkr: MCMC Sampling from Convex Polytopes}

\author{by Andy Yao, David Kane}

\maketitle      

\abstract{
Consider the intersection of two spaces. The first space is the complete solution space 
to $Ax=b$ (i.e. all vectors $x$ that satisfy $Ax=b$). The second space is the $N$-Simplex, 
described by $\sum\limits_{i=1}^N {x_i} = 1$ and $x_i \geq 0$. The intersection of 
these two spaces is a convex polytope. \pkg{walkr} samples from this intersection using 
two Monte-Carlo Markov Chain (MCMC) methods: hit-and-run and Dikin walk. \pkg{walkr}
also provide tools to examine the convergence properties of the random walks. 
% DESCRIPTION should mirror this abstract very similarly
}

\section{Introduction} 

Consider all possible vectors $x$ that satisfy the underdetermined matrix equation $Ax=b$,
where $A$ is a $M \times N$ matrix, $x$ is a $N \times 1$ vector, and $b$ is a $M \times 1$ vector. 
Assuming that the rows of $A$ are linearly indepedent (if not, then remove rows until linearly independent), 
the problem is only interesting when there are more rows than columns (more variables than equations, or $M < N$). 
If $M = N$, then there is only a single solution, and if $M > N$, there then are no solutions. 
When $M \ll N$, the problem is especially interesting, as the solution space is infinite and complicated. \\

Geometrically, every row in $Ax=b$ describes a hyperplane in $\mathbb{R}^N$. Therefore, $Ax=b$ represents
the intersection of hyperplanes in $\mathbb{R}^N$. 

$$
A_{M \times N} = 
\underbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
  \end{bmatrix}
}_\text{N columns (variables)}
\Bigg\}\text{{M rows (constraints)}}
$$

However, the intersection of hyperplanes described by $Ax=b$ is unbounded, as the solution space
streches to infinity. Therefore, we bound the sampling space by also requiring vector $x$
to be on the $N$-Simplex. The $\mathbf{N}$\textbf{-Simplex} is described by:

$$x_1 + x_2 + x_3 + ... + x_N = 1$$
$$x_i \geq 0, \qquad \forall i \in \text{\{$1, 2, ..., N$\}}$$ 

It is a $N-1$ dimensional object living in $N$ dimensional space. For example, the $3$D Simplex 
is a $2$ dimensional triangle in $3$D space. %(reference figure here). \\ %insert 3d simplex figure

The intersection of the complete solution to $Ax=b$ and the $N$-Simplex is a 
convex polytope. Sampling from such a convex polytope 
is a difficult problem, and the common approach is to run Monte Carlo Markov Chains (MCMC) 
in the interior of the polytope. \pkg{walkr} contains two MCMC random walks. Our first random walk is hit-and-run. 
Hit-and-run is a widely used MCMC sampling method that guarantees uniform sampling asympotically, 
but mixes slower and slower as the dimensions of $A$ increase (cite hitandrun here). 
Our second random walk is the Dikin Walk, which generates a nearly uniform sample and exhibits 
much stronger mixing (\cite{kannan}). \\

It is also important to know whether a MCMC chain has converged to a stationary
distribution or has mixed well. Often times, graphical tools such as 
trace-plots and statistical measures such as the Gelman-Rubin Statistical Tests 
serve as a valuable assessment of the chains. Therefore, \pkg{walkr} also provides
statistical diagnostics and visualization of the random walks through a wrapper 
to \pkg{shinystan}.

\section{Sampling Space: 3D Case}

Let's begin with the simplest case -- one linear constraint in 3 dimensional space.

$$x_1 + x_3 = 0.5$$

We can express this in terms of matrix equation $Ax=b$, where:

$$
A = 
\begin{bmatrix}
1 & 0 & 1 \\
\end{bmatrix},
\quad
b = 0.5,
\quad
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
$$ 

In figure~\ref{fig:3dcase}, we draw the intersection of the two. 
The orange equilateral triangle represents the 3D simplex, and the blue rectangle represents 
the plane $w_1+w_3=0.5$. The intersection of the hyperplane (blue) with the simplex (orange) 
is the red line segment, which is our sampling space.

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/3Dcase.png}
\caption[caption]{The orange triangle is the $3$D-simplex. The blue plane is the hyperplane $
                  x_1+x_3=0.5$. The red line is their intersection, which is the 
                  space we're interested in sampling from}
\label{fig:3dcase}
\end{figure} 

\section{Sampling Space: $4$D space}

Just like how the $3$D simplex is a $2$D surface living in $3$D space, the $4$D simplex 
(i.e. $x_1+x_2+x_3+x_4=1$, $x_i \geq 0$) could be viewed as a $3$D object (figure~\ref{fig:tetra1}). 
Specifically, the $4$D simplex is the following tetradhedron when viewed from $3$D space, with 
verticies $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$.\\

% Figure needs to be replaced
% FIGURE NEEDS TO LOOK MORE LIKE A EQUILATERAL 
\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra1.png}
\caption[caption]{The 4D simplex exists in 4D space, but can be viewed as a 3D object. Specifically, the
                  4D simplex is a tetrahedron, with its bottom as a equilateral triangle.}
\label{fig:tetra1}
\end{figure} 

Now imagine the intersection of the $4$D simplex with one hyperplane in $4$D (1 equation,
or 1 row in $Ax=b$). For a specific $A$ and $b$, we demonstrate the intersection 
in the figure below. The resulting shape is a trapezoid in $4$D space. 

$$
A = 
\begin{bmatrix}
1 & 1 & 1 & 1 \\
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
1\\
16
\end{bmatrix}
$$

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra2.png}
\caption[caption]{The 4D simplex is the tetrahedron. The hyperplane cuts through the tetrahedron, forming 
                  a trapezoid as the intersection (in red). This trapezoid is our sampling space, as 
                  it is the intersection of the hyperplane with the 4D simplex} %%% need to label points!!
\end{figure}

In higher dimensions, the same logic applies. 
Each row in $Ax=b$ is a hyperplane living in $\mathbb{R}^N$ (given $N$
variables). Thus, geometrically, our sampling space is: \textbf{the intersection of hyperplanes
with the $N$-simplex}. 

% There are 3 steps. first step is to combine simplex equality constraint into Ax = b
% second step is Ax = b solve for null space (we will abuse notation)
% third step is transforming to Ax <= b 
% 
% maybe show for example for 3D case

\section{From $x$-space to $\alpha$-space}

Our sampling space is a bounded (i.e. has finite volume in 
$\mathbb{R}^N$) convex polytope. In the literature, convex-polytopes are commonly
described by a generic $Ax \leq b$. In this section, we present
a 3 step procedure which transforms the intersection of $Ax=b$ and the $N$-simplex 
into the $Ax \leq b$ form (\textbf{note}: the $A$ and $b$ in $Ax \leq b$ is 
not the same as those which we began with in $Ax=b$. We use $A$ and $b$ 
in both occasions because it is standard notation in the literature for describing
both an underdetermined matrix equation and convex polytope.)

\subsection{Step 1: Combining Simplex Equality with $Ax=b$}

Recall that $A$ in $Ax=b$ is $M \times N$: 

$$
A_{M \times N} = 
\underbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
  \end{bmatrix}
}_\text{N columns (variables)}
\Bigg\}\text{{M rows (constraints)}}
$$

First, the equality part of the simplex constraint ($x_1+x_2+...+x_N=1$) could be 
added as an extra row in $Ax=b$. Let's call this new matrix $A'$:

$$
A' =
  \begin{bmatrix}
   & & ...... & & & \\
   & & ......  & & & \\
   & & ...... & & & \\
   1 & 1 & ...... & 1 & 1 \\

  \end{bmatrix}, 
\quad 
b' = 
  \begin{bmatrix}
  ... \\
  ... \\
  ... \\
  1 \\
  \end{bmatrix}
$$

\subsection{Step 2: Solving for the Null Space -- Transforming to Alpha-Space}

Second, we find all possible $x$'s that satisfy $A'x = b'$. To do so, we must first 
compute the Null-Space of $A'$ and then add on any particular solution that satisfy
$A'x=b'$ (For a review of finding Null Space and complete solution, one could 
refer to Leon's Linear Algebra textbook).\\ %(\cite{leon})) 

The Null Space of $A'$ can be represented by $N-(M+1)$ basis vectors, since we have
$N$ variables and $M+1$ constraints in $A'$. Every basis vector, $v_i$, has 
$N$ components:

$$
\text{basis vectors} = 
\Bigg\{
v_1, \quad v_2, \quad v_3, \quad ...... \quad , \quad v_{N-(M+1)}
\Bigg\}
$$

Once we have the basis vectors, we could express the set of all $x$'s that satisfy $A'x=b'$
in terms of coefficients $\alpha_i$. The intuition would be that the basis vectors
form a coordinate system in the complete solution space, and that the coefficients $\alpha_i$'s
represent linear combinations of basis vectors to cover the whole space. The complete
solution to $A'x=b'$ could be expressed as the set: 

$$
\Bigg\{ x = 
v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad | \quad \alpha_i \in \mathbb{R}
\Bigg\}
$$ \\

Here, it is very important to note that a \textbf{transformation} has taken place. The $v_i$ basis vectors 
and $v_{particular}$ are fixed vectors. The variable has turned from $x$'s in the original $Ax=b$ into 
coefficients $\alpha_i$'s. 

\subsection{Step 3: Including the Simplex Inequalities}

Lastly, we add the inequality constraints from the $N$-Simplex. Namely, we require 
every element of vector $x$ to be $\geq 0$: 

$$x = v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

Now, we express all coefficients $\alpha_i$ as a vector $\vec{\alpha}$:

$$\vec{\alpha} =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
... \\
\alpha_{N-(M+1)}
\end{bmatrix}
$$

We could also express the set of basis vectors as columns of matrix $V$:  

$$
V = 
\begin{bmatrix}
v_1 &
v_2 &
... &
v_{N-(M+1)} \\
\end{bmatrix}
$$

Therefore, the inequality now becomes:

$$
v_{particular} + V\vec{\alpha} \geq 
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

$$
V\vec{\alpha} \geq -v_{particular}
$$

$$
-V\vec{\alpha} \leq v_{particular}
$$

Voila! Here is the generic $Ax \leq b$ representation of a convex polytope (note again
that the $A$ and $b$ in $Ax \leq b$ are different from those in $Ax=b$). We have performed
a \textbf{transformation} from $x$-space to $\alpha$-space. In fact, \pkg{walkr} internally performs 
this transformation, samples the $\alpha$'s, then maps them back to "$x$-space. \\

The user need not be concerned with this transformation affecting the uniformity 
or mixing properties of our MCMC sampling algorithms. This is because the transformation
above is an affine transformation, which preserves uniformity.
Simply put, sampling in either space is equivalent. \\

\textbf{NOTE: SHOULD INCLUDE 3D EXAMPLE HERE}

\section{Sampling Algorithms}

\textbf{Important:} Before moving to sampling algorithms, we clear up on some nomenclature. 
First, as established above, the sampling space is a convex polytope. We shall refer to this convex 
polytope as $K$. Moreover, because it is very standard notation in the math and sampling literature to 
describe a convex polytope as $Ax \leq b$, we describe $K$ with $Ax \leq b$. However, we keep in mind that
the $A$, $x$, and $b$ are not the same as those in the original problem statement of $Ax=b$ and $N$-simplex. 
Instead, as described in the section above, we are actually sampling coefficients $\alpha$'s, and then
eventually transforming it back to $x$-space. Again, $Ax \leq b$ describe the polytope in terms of 
transformed coordinates $\alpha$.

\subsection{Picking Starting Points}

MCMC random walks need a starting point, $x_0$, in the convex polytope. \pkg{walkr}
generates such starting points using linear programming.
Specifically, the \texttt{lsei} function of the \pkg{limSolve} package (cite it) finds $x$ which: 

$$\text{minimizes} \quad |Cx-d|^2$$
$$\text{subject to} \quad Ax \leq b, \quad \text{this is our polytope $K$}$$

We randomly generate $C$ and $d$, obtaining points that minize the random objective function. 
Because we are performing a minimization, the points that are obtained this way often fall
onto the boundaries of our polytope $K$. Therefore, we repeat the process 30 times in \pkg{walkr}
and then take an average of those points. This averaged point ought to be still in the 
polytope $K$ because our polytope is convex. This procedure generates one starting point $x_0$. \\

Often times in MCMC random walks, it is beneficial to have multiple starting points at 
different locations in $K$. We call each random walk at a starting point a single chain.
For example, if we have 5 different starting points, each running its own random walk, 
we would have 5 chains. Once we have multiple chains, we could examine whether the chains have
all converged to the same stationary distribution. The correlation between different chains
is also of interest. Ideally, we hope to see a low correlation which reflects that
the chains are mixing well with each other.


\section{Random Walk: Hit-and-run}

We point out that the $Ax \leq b$ described below is the transformed polytope
in $\alpha$-space (discussed in section above). The $A$, $x$, and $b$ are different from
those in the original $Ax=b$ and $N$-Simplex. Once we sampled points, we perform the transformation
which maps the points back into the original $x$-space. 

\begin{enumerate}
  
  \item{Set starting point $x_0$ as current point}
  \item{Randomly generate a direction $\vec{d}$. If we are in $N$ dimensions, then $d$ will 
        be a vector of $N$ components. Specifically, $d$ is a uniformly generated 
        unit vector on the $N$ dimensional unit-sphere}
  \item{Find the chord $S$ through $x_0$ along the directions $\vec{d}$ and $-\vec{d}$.  
        We find end points $s_1$ and $s_2$ of the chord by going through the rows 
        of $Ax_0 \leq b$ one by one, setting the inequality to equality (so we hit the surface). 
        Then, parametrize the chord along $x_0$ by $s_1 + t(s_2-s_1)$, where $t \in [0,1]$}
  \item{Pick a random point $x_1$ along the chord $S$ by generating $t$ from \texttt{Uniform[0,1]}}
  \item{Set $x_1$ as current point}
  \item{Repeat algorithm until number of desired points sampled}
        
\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_trapezoid1.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun2.png}
\label{fig:subfigure1}}
\quad
\subfigure[Step 3]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun3.png}
\label{fig:subfigure2}}
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun4.png}
\label{fig:subfigure3}}
\quad
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun5.png}
\label{fig:subfigure4}}
%
\caption{The hit-and-run algorithm begins with an interior point $x_0$ (Step 1). A random direction
         is selected (Step 2), and the chord along that direction is calculated (Step 3).
         Then, we pick a random point along that chord and move there as our new point (Step 4).
         The algorithm is repeated to sample many points (Step 5)}
\label{fig:hitandrun}
\end{figure}

\pkg{walkr} uses the \code{har} function from the \pkg{hitandrun} package on CRAN (\cite{hitandrun}) 
to implement hit-and-run. \\

The hit-and-run algorithm has pros and cons. Describe pros and cons here (properties also).

\section{Random Walk: Dikin Walk}

Dikin walk is another method of MCMC random walks. In higher dimensions, the Dikin walk mixes much stronger
than hit-and-run does. blah blah explain more about why this is good/bad. ``At the cost of uniformity''. 

\subsection{Preliminary Definitions}

Recall, our sampling space is a convex polytope. We call this convex 
polytope $K$, which can be described in the form $Ax \leq b$. We point out that this $Ax \leq b$ 
is the transformed polytope in $\alpha$-space (discussed in transformation section above). 
The $A$, $x$, and $b$ are different from those in the original $Ax=b$ and $N$-Simplex.\\

For the definitions below, let $a_i$ represent a row in $A$, $x_i$, $b_i$ represent 
the $i^{th}$ element of $x$ and $b$. Also recall that $A$ is a $M \times N$ matrix. \\

\noindent \textbf{Log Barrier Function $\phi$:} 

$$\phi(x) = \sum {- \log(b_i - a_i^Tx)}$$

The log-barrier function of $Ax \leq b$ measures how extreme or ``close-to-the-boundary'' a point 
$x \in K$ is. This is because the value of $\phi$ tends to $\infty$ as $x$ tends to the 
boundary of the polytope $K$. Mathematically, the polytope could be described by: 

$$Ax \leq b$$
$$b - Ax \geq 0$$  %%% 3D CASE OF THE 

Thus, the higher the value of the log-barrier function ($\phi(x)$), the closer the point $x$ is to the boundary
of the convex polytope $K$ (as $(b_i-a_ix) \rightarrow 0$). \\

\noindent \textbf{Hessian of Log Barrier $H_x$:} 

$$H_x = \nabla ^2 \phi(x) = ...... = A^T D^2 A \quad , \quad \text{where:}$$
$$D = diag(\frac{1}{b_i - a_i^Tx})$$

\textbf{Note:} $H_x$ is a $N \times N$ linear operator. $D$ is a $M \times M$ diagonal matrix.\\

The Hessian matrix($H_x = \nabla ^2 \phi(x)$) contains all the second derivatives 
of the function $\phi(x)$ with respect to vector $x$. In the land of optimization, 
the Hessian contains information on how the landscape is shaped, and also on extreme values of 
the landscape. To develop a better understanding of our intuition for the Hessian, we 
can think of it as a generalized notion of the second derivative. \\

Let $z$ denote $b_i - a_ix$.
Note that the second derivative of $-\log(z)$ is $\frac{1}{z^2}$, which looks like this: \\

As we could see from the plot, as $z \rightarrow 0$ ($x \rightarrow \text{boundary}$), the rate at 
which the Hessian increases becomes faster and faster. %Thus, the Hessian of the log-barrier is 
%a function that quantifies how quickly 

\noindent \textbf{Dikin Ellipsoid $D_{x_0}^r$} \\

Define $D_{x_0}^r$, the Dikin Ellipsoid centered at $x_0$ with radius $r$ as: 

\begin{center}
$D_{x_0}^r \quad = \quad $\{$y \quad | \quad (y-x_0)^T H_{x_0}(y-x_0) \leq r^2$\} 
\end{center}

The Dikin Ellipsoid with radius $r = 1$ is the unit ball centered at $x_0$ with respect to the 
"Hessian norm" as the notion of length. The "Hessian norm" we could think as a $\delta y$ on 
the previous $\frac{1}{z^2}$ graph. For $x_0$ that are far away from the boundary, given
an allowed $\delta y$ (captured in $r$), the range for allowed value of $z$ is very large.
This corresponds to having a large ellipsoid. \\

To rephrase, the Dikin Ellipsoid is the collection of all points whose difference with 
the current point($y-x_0$) is within the unit threshhold of $r=1$. When the center $x_0$ of the Dikin Ellipsoid 
is far from the boundary of the polytope, the ellipsoid is larger, as a large $\delta (z-x)$ corresponds
to a small $\delta y$. As the center point $x_0$ approaches the boundary 
of the polytope, the ellipsoid becomes smaller and smaller. Therefore, the ellipsoid is able to reshape 
itself as it surveys through the polytope $K$, biasing away from the corners of the region.

\subsection{Algorithm \texttt{Dikin}}

\begin{enumerate}

  \item{Begin with a point $x_0 \in K$. This starting point must be in the polytope.}
  \item{Construct $D_{x_0}$, the Dikin Ellipsoid centered at $x_0$}
  \item{Pick a random point $y$ from $D_{x_0}$}
  \item{If $x_0 \notin D_{y}$, then reject $y$ (be careful, this condition is counter-intuitive)}
  \item{If $x_0 \in D_{y}$, then accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ 
        (the big picture is that the ratio of the determinants are equal
         to the ratio of volumes of the ellipsoids centered at $x_0$ and $y$. Thus, 
         the geometric argument would be that 
         this way the Dikin walk can avoid extreme corners of the region)}
  \item{repeat until obtained number of desired points}

\end{enumerate}
\newpage

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin2.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin3.png}
\label{fig:subfigure2}}
\quad
\subfigure[Step 3 Case I]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin41.png}
\label{fig:subfigure3}}
\subfigure[Step 3 Case II]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin42.png}
\label{fig:subfigure4}}
\quad
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin5.png}
\label{fig:subfigure5}}
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin6.png}
\label{fig:subfigure6}}

%
\caption{The Dikin Walk begins by constructing the Dikin Ellipsoid at the starting point $x_0$ (Step 1). 
         An uniformly random point $y$ is generated in the Dikin Ellipsoid centered at $x_0$ (Step 2). 
         If point $x_0$ is not in the Dikin Ellipsoid centered at $y$, then reject $y$ (Step 3 Case I).
         If point $x_0$ is contained in the Dikin Ellipsoid centered at $y$, then 
         accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ (Step 3 Case II).
         Once we've successfully accepted $y$, we set $y$ as our new point, $x_1$ (Step 4). 
         Algorithm repeats (Step 5)}
         
\label{fig:figure}
\end{figure}

%' \section{Using \texttt{walkr}} %%% MADE FLOW BETTER 
%' 
%' %% this is helpful : http://yihui.name/knitr/options/
%' 
%' \pkg{walkr} has one main function \code{walkr} which samples points. \\
%' 
%' First, the user must specify the $A$ and $b$ in the $Ax=b$ hyperplanes equation. For example,
%' for a 5 dimensional case:
%' 
%' <<example2, eval = T>>=
%' A <- matrix(c(1, 0, 1, 0, 1), ncol = 5)
%' b <- 0.5
%' @
%' 
%' The \texttt{walkr} function takes care of the intersection with the simplex for the user. However,
%' if the user just want to sample from the simplex with no hyperplanes, she can simply specify the
%' simplex equation, as \texttt{walkr} will remove linearly dependent rows internally.
%' 
%' <<example3, eval = FALSE>>=
%' A <- matrix(1, ncol = 5)
%' @
%' 
%' After we have \code{A} and \code{B}, we are now ready for sampling. \texttt{walkr} has the following
%' parameters:
%' 
%' \begin{itemize}
%' 
%'   \item{ \code{A} is the right hand side of the matrix equation $Ax=b$}
%'   \item{ \code{b} is the left hand side of the matrix equation $Ax=b$}
%'   \item{ \code{method} is the method of sampling -- can be either \code{"hit-and-"run"} or 
%'          \code{"dikin"}}
%'   \item{ \code{thin} is the thinning parameter. Every \code{thin}-th point is stored into the final sample}
%'   \item{ \code{burn} is the burning parameter. The first \code{burn} points are deleted from the final sample}
%'   \item{ \code{chains} is the number of chains we want to sample. Every chain is an element 
%'         of the list which \code{walkr} eventually returns}
%'   \item{ \code{ret.format} is the return format of the sampled points. If \code{"list"}, then
%'          a list of chains is returned, with each chain as a matrix of points. If \code{"matrix"}, 
%'          then a single matrix of points is returned. A column is a sampled point.}
%' 
%' \end{itemize}
%' 
%' \code{walkr} warns the user if the chains have not mixed "well-enough" or have not converged
%' to a stationary distribution according to the Gelman-Rubin Diagnostics (\cite{gelman}). 
%' 
%' <<example4, eval = TRUE>>=
%' ## sampling from the 3D simplex
%' library(walkr)
%' A <- matrix(1, ncol = 3)
%' b <- 1
%' answer <- walkr(A = A, b = b, points = 10, method = "hit-and-run", 
%'                 thin = 1, burn = 0, chains = 5, ret.format = "list")
%' answer
%' @
%' 
%' As we see from the code chunk above, \code{walkr} returns a warning message that the $\hat{R}$ are not satisfying, since we only have 10 points. However, if we run the chain for long (or increasing thinning), 
%' then the statistical tests pass.
%' 
%' <<example5, eval = TRUE>>=
%' answer <- walkr(A = A, b = b, points = 1000, method = "hit-and-run", 
%'                 thin = 1, burn = 0, chains = 5, ret.format = "list")
%' @
%' 
%' Moreover, \code{walkr} provides an easy to use wrapper function \code{explore\_walkr} which
%' calls \code{shinystan} to diagnose and visualize the MCMC chains. Note that the \code{"ret.format"}
%' must be a \code{"list"}.
%' 
%' <<example5_5, eval = FALSE>>=
%' explore_walkr(answer)
%' @
%' 
%' %% ATTACH SCREENSHOT HERE
%' 
%' \section{Dikin versus Hitandrun}
%' %reference here:{http://www.mit.edu/~har/Dikin.pdf}
%' \begin{tabular}{ |p{3cm}|p{5cm}|p{5cm}|}
%'   \hline
%'    &  \texttt{hit-and-run} & \texttt{Dikin Walk} \\ \hline
%'   Uniform Sampling & Yes, needs $O(N^3)$ points, where $N$ is the dimension of the polytope & 
%'   No, concentrates in the    interior\\ \hline
%'   Mixing & $O(\frac{N^2R^2}{r^2})$ *, slows down substantially as dimension of polytope increases and 
%'   polytope becomes "skinnier" & $O(MN)$, where $A$ is $M \times N$; much stronger mixing. \\ \hline
%'   Cost of One Step & $O(MN)$ & $O(MN^2)$, in practice, one step of Dikin is much more 
%'   costly than hit-and-run\\ \hline
%'   Rejection Sampling & No & Yes (see probability formula and $x \notin D_y$), but rejection rate 
%'   not high\\ \hline
%' 
%' \end{tabular} \\ 
%' 
%' *$R$ is the radius of the smallest ball that contains the polytope $K$. $r$ is the radius of the 
%' largest ball that is contained within the polytope $K$. Thus, $\frac{R}{r}$ increases 
%' as the polytope is "skinnier"(\cite{kannan}). \\
%' 
%' As we can in the two trace-plots below, the mixing for Dikin is much better than hit-and-run
%' given the same set of parameters
%' 
%' <<example7, eval = TRUE, fig.width=5, fig.height=3>>=
%' set.seed(314)
%' N <- 50
%' A <- matrix(sample(c(0,3), N, replace = T), nrow = 1)
%' A <- rbind(A, matrix(sample(c(0,3), N, replace = T), nrow = 1))
%' b <- c(0.7, 0.3)
%' 
%' answer_hitandrun <- walkr(A = A, b = b, points = 500, method = "hit-and-run", 
%'                  thin = 10, burn = 0, chains = 1)
%' answer_dikin <- walkr(A = A, b = b, points = 500, method = "dikin", 
%'                  thin = 10, burn = 0, chains = 1)
%' 
%' plot(y = answer_hitandrun[50,], x = 1:500, 
%'      xlab = "random walk", ylab = "value", type = 'l', 
%'      main = "Hit-and-run Mixing",
%'      ylim = c(0, 0.2))
%' plot(y = answer_dikin[50,], x = 1:500, 
%'      xlab = "random walk", ylab = "value", type = 'l',
%'      main = "Dikin Mixing", 
%'      ylim = c(0, 0.2))
%' @
%' 
%' As mentioned before, Dikin samples more in the interior of the polytope $K$.
%' 
%' % # <<example8, eval = TRUE, fig.width = 3, fig.height = 4>>=
%' % # A <- matrix(c(2,0,3,0), ncol = 4)
%' % # b <- 0.5
%' % # set.seed(314)
%' % # z1 <- walkr(A = A, b = b, points = 1000, method = "dikin")
%' % # z2 <- walkr(A = A, b = b, points = 1000, method = "hit-and-run")
%' % # library(scatterplot3d)
%' % # scatterplot3d(x = z1[2,], y = z1[3, ], z = z1[4, ], pch = 19, scale.y = 2, main = "Dikin")
%' % # scatterplot3d(x = z2[2,], y = z2[3, ], z = z2[4, ], pch = 19, scale.y = 2, main = "Hit-and-Run")
%' % # @
%' 
%' 
%' \section{Conclusion}
%' 
%' \code{walkr} samples $x$ that satisfy the underdetermined matrix equation $Ax=b$, every component
%' of $x$ is non-negative, and every component sum to $1$. \code{walkr} internally performs the affine
%' transformation which transforms $Ax=b$ and the simplex constraints into the $Ax \leq b$ form. Then, 
%' \code{walkr} samples points under this transform, takes the inverse transform, and finally
%' returns the $x$'s sampled.
%' 
%' The two methods that the user may specify is \code{"dikin"} and \code{"hit-and-run"}. Hit-and-run
%' is a widely used sampling method that converges to the uniform sampling asymptotically, but 
%' fails to mix well in higher dimensions. Dikin generates a nearly uniform sample, but mixes stronger than
%' hit-and-run, especially in higher dimensions. In addition, \code{walkr} will warn the user if the 
%' mixing is not strong enough according to the Gelman-Rubin Diagnostics.

%% world is not perfect? no. ideally we would like something to do blah blah blah 

% \section{Authors}
% 
% \address{Andy Yao\\
% Mathematics and Physics\\
% Williams College \\
% Williamstown, MA, USA\\}
% \email{andy.yao17@gmail.com}
% 
% \address{David Kane\\
% Managing Director \\
% Hutchin Hill Capital\\
% 101 Federal Street, Boston, USA\\}
% \email{dave.kane@gmail.com}
% 
\bibliography{walkr}

\end{article}
\end{document}