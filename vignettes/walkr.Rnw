\documentclass[a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage[round]{natbib}
\usepackage{subfigure}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{walkr}

\begin{document}

%% do not edit, for illustration only
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{YY}
\year{20ZZ}
\month{AAAA}

\begin{article}

\title{walkr: MCMC Sampling from Non-Negative Convex Polytopes}

\author{by Andy Yao, David Kane}

\maketitle      

\abstract{
Consider the intersection of two spaces: the complete solution space to $Ax=b$ and the $N$-simplex, 
described by $\sum\limits_{i=1}^N {x_i} = 1$ and $x_i \geq 0$. The intersection of 
these two spaces is a non-negative convex polytope. The \code{R} package \pkg{walkr} samples from this 
intersection using two Monte-Carlo Markov Chain (MCMC) methods: hit-and-run and Dikin walk. \pkg{walkr}
also provides tools to examine sample quality.
}

\section{Introduction} 

Consider all possible vectors $x$ that satisfy the matrix equation $Ax=b$,
where $A$ is $M \times N$, $x$ is $N \times 1$, and $b$ is $M \times 1$. 
The problem is interesting when there are more rows than columns  ($M < N$). 
If $M = N$, then there is a single solution, and if $M > N$, there then are, in general, no solutions.
If the rows of $A$ are linearly dependent, rows can be eliminated until they are linearly 
independent without changing the solution space. Assume that the rows of $A$ are linearly independent
going forward.

Geometrically, every row in $Ax=b$ describes a hyperplane in $\mathbb{R}^N$. Therefore, $Ax=b$ represents
the intersection of $M$ unbounded hyperplanes. We bound the sample space by also 
requiring vector $x$ to be in the $N$-simplex, defined as: 

$$x_1 + x_2 + x_3 + ... + x_N = 1$$
$$x_i \geq 0, \qquad \forall i \in \text{\{$1, 2, ..., N$\}}$$ 

The $N$-simplex is a $N-1$ dimensional object living in $N$ dimensional space. For example, the $3$D simplex 
is a $2$ dimensional equilateral triangle in $3$D space (Figure~\ref{fig:simplex3D}).

\begin{figure}[H]
\centering
\includegraphics[width = 3.5in, height = 2.5in]{img/simplex3D.png}
\caption[caption]{The $3$D simplex is a $2$ dimensional triangle in $3$ dimensional space. The vertices
                  of the simplex are $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$. $x_1$, $x_2$, and $x_3$ 
                  are all greater than or equal to $0$, and for all points on the simplex, the sum
                  of $x_1$, $x_2$ and $x_3$ equals $1$.}
\label{fig:simplex3D}  
\end{figure} 

The intersection of the complete solution of $Ax=b$ and the $N$-simplex is a non-negative
convex polytope. Sampling from such an object is a difficult problem, and the common approach is to 
run Monte-Carlo Markov Chains (MCMC) (\cite{ravi}). MCMC methods begin at a starting point within
the sample space (in our case, a non-negative convex polytope), and randomly ``wanders'' through
the sample space according to an algorithm. An important feature of MCMC is that every step
only depends on the previous step and nothing else. MCMC generally involve the creation
of multiple random walks from different starting points, each of which is an independent ``chain''.
A key aspect of running multiple chains from different starting points is to examine the 
``mixing'' of the different chains. Good mixing means that the different chains have thoroughly 
moved around the sample space and have overlapped with each other. It is important to note that
while good mixing does not guarantee a complete, uniform sample, 
poor mixing definitely points at an unthorough survey of the space. \pkg{walkr} allows the user to 
examine the quality of the MCMC samples.

\pkg{walkr} includes two MCMC algorithms: hit-and-run and Dikin walk.
Hit-and-run guarantees uniform sampling asympotically, but mixes more slowly
as the number of columns in $A$ increase (\cite{vempala}). Dikin walk generates a ``nearly'' uniform
sample --- favoring points away from the edges of the polytope --- but exhibits much faster mixing (\cite{kannan}). 

\section{Three dimensional example}

Consider one linear constraint in three dimensions.

$$x_1 + x_3 = 0.5$$

We can express this in terms of the matrix equation $Ax=b$:

$$
A = 
\begin{bmatrix}
1 & 0 & 1 \\
\end{bmatrix},
\quad
b = 0.5,
\quad
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
$$ 

Figure~\ref{fig:3dcase} shows the intersection of the $3$D simplex with $Ax=b$. 

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/3Dcase.png}
\caption[caption]{The orange triangle is the $3$D-simplex. The blue plane is the hyperplane $
                  x_1+x_3=0.5$. The red line segment is their intersection, which is our sample space. 
                  The end points of the line segment are $(0,0.5,0.5)$ and $(0.5,0.5,0)$.}
\label{fig:3dcase} 
\end{figure} 

\section{Four dimensional example}

Just as the $3$D simplex is a $2$D surface living in $3$D space, the $4$D simplex 
(i.e. $x_1+x_2+x_3+x_4=1$, $x_i \geq 0$) can be viewed as a $3$D object, as in Figure~\ref{fig:tetra1}. 
Specifically, the $4$D simplex is a tetradhedron when viewed from $3$D space, with 
verticies $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$.

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra1.png}
\caption[caption]{The 4D simplex exists in 4D space, but can be viewed as a 3D object. Specifically, the
                  4D simplex is a tetrahedron, with all four sides equilateral triangles.} 
\label{fig:tetra1}
\end{figure} 

Figure~\ref{fig:tetra2} shows the intersection of the $4$D simplex with a specific
hyperplane (1 equation, or 1 row in $Ax=b$). The resulting convex polytope is a 2D trapezoid in $4$D space. 
Note that the convex polytope is $4-(1+1)=2$ dimensions. This is because we began with $4$ dimensions,
and the constraint and the simplex each reduced the dimension of the solution space by 1. 
In general, the dimensions of our polytope will be $N-(M+1)$ dimensional, where the $+1$ comes from
adding on the $N$-simplex. 

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra2.png}
\caption[caption]{The 4D simplex is the tetrahedron. The hyperplane $22x_1+2x_2+2x_3+37x_4=16$ cuts through 
                  the tetrahedron, forming 
                  a trapezoid as the intersection (in red). This trapezoid is our sample space, as 
                  it is the intersection of the hyperplane with the 4D simplex. The vertices
                  of the trapezoid are ($0.7,0.3,0,0$), ($0.7,0,0.3,0$), ($0,0.6,0,0.4$),
                  and ($0,0,0.6,0.4$).} 
\label{fig:tetra2}
\end{figure}

In higher dimensions, the same logic applies. Each row in $Ax=b$ is a hyperplane living in $\mathbb{R}^N$.
Geometrically, our sample space is the intersection of $M$ hyperplanes with the $N$-simplex, 
which will have dimension $N-(M+1)$. 

\section{$x$-space and $\alpha$-space}

Our sample space is a bounded, non-negative convex polytope. In the literature, convex polytopes 
are commonly described by a generic $Ax \leq b$. In order to use the sampling algorithms, we must
first re-express the sample space in the form $Ax \leq b$ (with different $A$, $x$ and $b$)
\footnote{
This is a total abuse of notation. The $A$ in $Ax=b$ is very different from the $A$ in
$Ax \leq b$. The length of $x$ in $Ax = b$ is $N$. The length of $x$ in $Ax \leq b$ is $N-(M+1)$.
$b$ is also different. The mathematical literature for linear equations uses $Ax = b$, and the litearture
on convex polytopes uses $Ax \leq b$, so it seemed best to use the same notation in both places 
in order to make the connections to existing litearture easier.
}. 

Recall that our sample space is the intersection of the complete solution to $Ax=b$ and 
the $N$-Simplex, which could be represented by three parts. First, the matrix equation 
$Ax=b$. Second, the simplex equation $x_1+x_2+...+x_N=1$. Third, the simplex 
non-negative inequalities, $x_i \geq 0$. In this section, we present a 3 step procedure
to combine all three parts into one single inequality of the form $Ax \leq b$. 

\subsection{Step 1: combining simplex equality with the original $Ax=b$}

Recall that $A$ in $Ax=b$ is $M \times N$: 

$$
A_{M \times N} = 
\overbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
  \end{bmatrix}
}^\text{N columns}
\Bigg\}\text{{M rows}}
$$

Add an extra row in $Ax=b$ which captures the equality part of the simplex constraint ($x_1+x_2+...+x_N=1$).
Call this new matrix $A'$:

$$
A' =
  \begin{bmatrix}
   & &  & & & \\
   & & A  & & & \\
   & &  & & & \\
   1 & 1 & ...... & 1 & 1 \\

  \end{bmatrix}, 
\quad 
b' = 
  \begin{bmatrix}
   \\
  b \\
   \\
  1 \\
  \end{bmatrix}
$$

\subsection{Step 2: solving for the null space -- transforming to $\alpha$-space}

Find $x$'s that satisfy $A'x = b'$. First, solve for the null space of $A'$, 
defined as all $x$ that satisfy $A'x=0$. The null space is 
spanned by $N-(M+1)$ basis vectors, because $A'$ is $M+1 \times N$ ($M+1 < N$) 
and hence the dimension of the null space is $N-(M+1)$. Any vector formed by 
a linear combination of the basis vectors will still be in the null space. 

Once we have the null space, we add on a particular solution to the null space. 
The null space we can think of as constructing a coordinate
system for $A'x=0$', and the particular solution to $A'x=b'$ we can think of as an offset from the origin 
of that coordinate system. For a review of the specifics of finding null spaces and particular solutions,
one can refer to \cite{leon}.

The null space of $A'$ can be represented by $N-(M+1)$ basis vectors. Because we are in $\mathbb{R}^N$, 
every basis vector, $v_i$, has $N$ components:

$$
\text{basis vectors} = 
\Bigg\{
v_1, \quad v_2, \quad v_3, \quad ...... \quad , \quad v_{N-(M+1)}
\Bigg\}
$$

Once we have the null space basis vectors and a particular solution, 
we could express the set of all $x$'s that satisfy $A'x=b'$
in terms of coefficients $\alpha_i$. The complete
solution to $A'x=b'$ could be expressed as the set: 

$$
\Bigg\{ x = 
v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad | \quad \alpha_i \in \mathbb{R}
\Bigg\}
$$ 

\subsection{Step 3: including the simplex inequalities}

We add the inequality constraints from the $N$-simplex, requiring every element of vector $x$ to be $\geq 0$: 

$$x = v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

We express all coefficients $\alpha_i$ as a vector $\alpha$:

$$\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
... \\
\alpha_{N-(M+1)}
\end{bmatrix}
$$

We can also express the set of basis vectors as columns of matrix $V$:  

$$
V = 
\begin{bmatrix}
v_1 &
v_2 &
... &
v_{N-(M+1)} \\
\end{bmatrix}
$$

Therefore, the inequality now becomes:

$$
v_{particular} + V\alpha \geq 
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

$$
V\alpha \geq -v_{particular}
$$

$$
-V\alpha \leq v_{particular}
$$

Finally arrive at the desired convex polytope form $Ax \leq b$. 

\subsection{Four dimensional transformation example}

Consider a four dimensional example drawn from Figure~\ref{fig:tetra2}. 

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\textbf{Step 1:} Add an extra row in $Ax=b$ to capture the equality part of the simplex constraint.

$$
A' = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
1 & 1 & 1 & 1 \\
\end{bmatrix},
\quad
b' = 
\begin{bmatrix}
16 \\
1 \\
\end{bmatrix}
$$

\textbf{Step 2:} The null space basis contain 2 vectors, as $M-(N+1) = 4 - (1+1) = 2$ is the dimension of our solution space.
The null space basis vectors (to three decimal places) are:

$$
v_1 = 
\begin{bmatrix}
-0.103 \\
-0.680 \\
0.723 \\
0.059 \\
\end{bmatrix},
\quad
v_2 = 
\begin{bmatrix}
-0.833 \\
0.265 \\
0.092 \\
0.476
\end{bmatrix}
$$

A particular solution to $A'x=b'$ is (any particular solution would work):

$$
v_{particular} = 
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
$$

\textbf{Step 3:} We add on the simplex inequalities:

$$
v_{particular} + \alpha_1 v_1 + \alpha_2 v_2 
\geq
\begin{bmatrix}
0 \\
0 \\
0 \\ 
0 \\
\end{bmatrix}
$$

Finally, we re-express the inequalities as $-V\alpha \leq v_{particular}$, which is of the form $Ax \leq b$

$$
V = 
\begin{bmatrix}
-0.103 & -0.833\\
-0.680 & 0.265\\
0.723 & 0.092\\
0.059 & 0.476\\
\end{bmatrix}
$$

$$
\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
$$

$$
\begin{bmatrix}
0.103 & 0.833\\
0.680 & -0.265\\
-0.723 & -0.092\\
-0.059 & -0.476\\
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
\leq
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
$$

$$
-V\alpha \leq
v_{particular}
$$

We sample $\alpha$'s according to our MCMC sampling algorithms below. Then,
we can map the $\alpha$'s back as $x$'s in the original coordinate system by 
$$
x = 
v_{particular} + 
V\alpha
$$

For example, an $\alpha$ we sample could be:

$$
\alpha = 
\begin{bmatrix}
-0.149 \\
-0.372 \\
\end{bmatrix}
$$

Apply map to $\alpha$, obtaining $x$ in the original problem statement. 

$$
x = 
v_{particular}+V\alpha
=
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
+
\begin{bmatrix}
-0.103 & -0.833\\
-0.680 & 0.265\\
0.723 & 0.092\\
0.059 & 0.476\\
\end{bmatrix}
\begin{bmatrix}
-0.149 \\
-0.372 \\
\end{bmatrix}
=
\begin{bmatrix}
0.539 \\
0.152 \\
0.219 \\
0.090 \\
\end{bmatrix}
$$

Indeed, the mapped point satisfies the original $Ax=b$ and the $N$-simplex equation and inequalities. 

Recall that we began with the intersection
of the complete solution to $Ax = b$ and the $N$-Simplex, represented as three different parts which 
we dealt with individually. First, we bring in the $x_1+x_2+...+x_N=1$ part of the simplex equation 
into the matrix equation $Ax=b$. Second, we solve for a particular solution 
and the the null space of $A$, obtaining the matrix $V$ which 
contains the basis vectors of the null space of $A$. Finally, we add the inequality constraints $x_i \geq 0$ 
to obtain $-V\alpha \leq v_{particular}$. 

Going forward, we will not use the $-V \alpha \leq v_{particular}$ notation and will instead
use the notation $Ax \leq b$. This is a total abuse of notation, as the $A$, $x$ and $b$ 
in $Ax \leq b$ are actually $-V$, $\alpha$ and $v_{particular}$, which is completely 
different from the $A$ and $b$ in our initial problem statement. We do this because in the 
convex polytopes literature, it is standard to represent the polytope as $Ax \leq b$, so
it seemed best to use the same notation in order to make the connectiosn to existing literature easier. 

\section{Algorithms}

\textbf{Important:} Before moving to sampling algorithms, we restate our problem.
Define non-negative convex polytope $K$ to be the solution to $Ax \leq b$
\footnote{
Again, the $Ax \leq b$ is different from the $Ax = b$ we began with.
For a detailed explaination on how we went from $Ax = b$ and the $N$-simplex
to $Ax \leq b$, with different $A$, $x$, $b$, see the previous section. 
}. We are interested in sampling from $K$.

The two Monte-Carlo Markov Chains (MCMC) sampling methods we implement are hit-and-run
and Dikin walk. MCMC methods begin at a starting point in $K$ and wanders through $K$
according to our algorithm. Every MCMC step only depends on the previous step and nothing else.
Ideally, we want the sample to thoroughly go through the sample space. 

To examine the quality of our sample, we create multiple, independent ``chains'' from 
different starting points in $K$ and observe their ``mixing''. The chains have mixed well 
if the parameters of different chains have thoroughly moved around the sample space and 
have overlapped with each other. In this case, sample quality is good. If the chains
have not mixed well, then we need to run the chains for longer. While good mixing does 
not guarantee that the sampling is perfect, poor mixing definitely points at an unthorough 
sample. %% need to correct more...

\subsection{Starting point}

MCMC random walks need a starting point, $x_0$ in $K$. \pkg{walkr}
generates starting points using linear programming.
Specifically, the \texttt{lsei} function of the \pkg{limSolve} package (\cite{limsolve}) finds $x$ which: 

$$\text{minimizes} \quad |Cx-d|^2$$
$$\text{subject to} \quad Ax \leq b, \quad \text{this is our polytope $K$}$$

We randomly generate matrix $C$ and vector $d$. Solving this system generates an $x$ 
which will usually fall on the boundaries of polytope $K$. We repeat this process 30 times in 
\pkg{walkr} and take an average of those points, which generates one starting point $x_0$. 


\section{Hit-and-run} 

\cite{vempala} provides an overview of the hit-and-run algorithm, as follows:

\begin{enumerate}
  
  \item{Set starting point $x_0$ as current point.}
  \item{Generate a random direction $d$ from the $N$ dimensional unit-sphere.}
  \item{Find the chord $S$ through $x_0$ along the directions $d$ and $-d$. 
        Define end points $s_1$ and $s_2$ as the intersection of the chord $S$
        with the edges of $K$. Because $K$ is convex, we chord $S$ will only 
        intersect it at two points. Parametrize the chord along $x_0$ by $s_1 + t(s_2-s_1)$, 
        where $t \in [0,1]$.}
  \item{Pick a random point $x_1$ along the chord $S$ by generating $t$ from \texttt{Uniform[0,1]}.}
  \item{Set $x_1$ as current point.}
  \item{Repeat algorithm until number of desired points sampled.}
        
\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_trapezoid1.png}
\label{fig:subfigure1}}
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun2.png}
\label{fig:subfigure1}}
\quad
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun3.png}
\label{fig:subfigure2}}
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun4.png}
\label{fig:subfigure3}}
\quad
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun5.png}
\label{fig:subfigure4}}
%
\caption{The hit-and-run algorithm begins with an interior point $x_0$ (a). A random direction  
         is selected (b), and the chord along that direction is calculated (c).           
         Then, we pick a random point along that chord and move there as our new point (d).
         The algorithm is repeated to sample many points (e).} 
\label{fig:hitandrun}
\end{figure}

\pkg{walkr} uses the \code{har} function from the \pkg{hitandrun} package(\cite{hitandrun})
to implement hit-and-run. The hit-and-run algorithm asymptotically generates an uniform
sample in the convex polytope $K$. However, the mixing of hit-and-run becomes slower 
with increasing dimensions of $K$. In practice, this slow mixing behavior of hit-and-run
is observed when we run multiple chains from different starting points. Instead of 
overlapping with each other, the individual chains stay at their initial
values. This behavior corresponds to ``getting stuck in corners'' of a ``skinny''
polytope. To solve this problem, we must let the individual chains run for longer(i.e. 
sample more points). However, as the dimensions of the polytope increase,
the number of points we need to sample become infeasible.  %%% should correct this more



\section{Dikin walk}

A Dikin walk is the second of two MCMC methods implemented in the \pkg{walkr} package.
A Dikin walk begins from a random starting point within the convex polytope $K$ 
and then creates a Dikin ellipsoid centered at the current point. It then samples
from the Dikin ellipsoid and if the next step is not extreme, it will mark that as the next
step. %% correct
The shape of the Dikin ellipsoid depends on $Ax \leq b$. The closer a point is to the edge, 
the small the Dikin ellipsoid, ensuring that the Dikin ellipsoid does not leave polytope 
$K$. Unlike hit-and-run, the Dikin walk does not sample uniformly throughout $K$. Instead,
it is biased towards points that are way from the edges and corners of $K$ (\cite{kannan}). %should i cite this random fact anyways?
This bias allows the chains to mix more quickly than hit-and-run, and therefore, 
to work better in higher dimensions. 

For non-negative convex polytope $K$, defined as all $x$ for which $Ax \leq b$, define $a_i$ 
as the $i^\text{th}$ row of $A$. Define $x_i$ and $b_i$ as the $i^\text{th}$ element of $x$ and $b$ 
respectively. The dimensions of $A$ are $m = N$ by $n = N-(M+1)$, where $M$ and $N$ are the dimensions
of $A$ in $Ax=b$, the original statement of our problem. 

\noindent \textbf{Log Barrier Function $\phi$:} 

$$\phi(x) = \sum {- \log(b_i - a_i^Tx)}$$

The log-barrier function of $Ax \leq b$ measures how extreme or ``close-to-the-boundary'' a point 
$x \in K$ is, because the negative $\log$ function tends to infinity as its argument tends to zero.
Since $Ax \leq b$, then for every row $a_i$ and $b_i$, $b_i > a_i^Tx$. Therefore, 
as $x$ approaches the boundary of $K$ (as $a_i^Tx$ approaches $b_i$), the value of $\phi$
approaches infinity. Hence, this is a barrier function, as the value of $\phi$ is 
infinity of the edge of $K$. It is also exactly because of this that the starting point cannot
on the boundary of $K$, where $b_i = a_i^Tx$, but be in the interior of $K$. 

\noindent \textbf{Hessian of Log Barrier $H_x$:} 

$$H_x = \nabla ^2 \phi(x) = ...... = A^T D A, \quad \text{where:}$$
$$D = diag(\frac{1}{{b_i - a_i^Tx}^2})$$  

$H_x$ is a $n \times n$ linear operator. $D$ is a $m \times m$ diagonal matrix. 
The Hessian matrix ($H_x$) contains the second derivatives of the function $\phi(x)$ with respect 
to the vector $x$. The Hessian describes the shape of the landscape. %% need to send email 

%% maybe describe more about the hessian
%% the cloes you are to the edge, the individual elemetns become
%% very large 

%% point 1: the closer you are to the edge, the diag elements of the 
%% point 2: 

%% explore more about the determinant, the intuition

%%1. hessian as the norm
%%2. hessian as volume of ellipsoid 

\noindent \textbf{Dikin Ellipsoid $D_{x_0}^r$} \\

Define the Dikin Ellipsoid, $D_{x_0}^r$, centered at $x_0$ with radius $r$ as: 

\begin{center}
$D_{x_0}^r \quad = \quad $\{$y \quad | \quad (y-x_0)^T H_{x_0}(y-x_0) \leq r^2$\} 
\end{center}

The Dikin Ellipsoid with radius $r = 1$ is the unit ball centered at $x_0$ with respect to the 
``Hessian norm'' as the notion of length. The ``Hessian norm'' we could think as a $\Delta \frac{1}{z^2}$ on 
the $\frac{1}{z^2}$ graph. For $x_0$'s that are far away from the boundary, given
an allowed $\Delta \frac{1}{z^2}$ (captured in $r$), the range of allowed values of $y-x_0$ is very large.
This corresponds to having a large Dikin ellipsoid. Alternatively, if $x_0$ is near the boundary,
then given an allowed $\Delta \frac{1}{z^2}$, only a small range of $y-x_0$ is acceptable -- a smaller
Dikin ellipsoid. %%% this paragraph should be rewritten once we fix the intuition above
%% leave note to self: maybe add the 4D stuff


%% GO THROUGH ALL THE MATH WE TALKED ABOUT ABOVE IN THIS NEW SPACE 
%% the hessian at x and hessian at y 
%% and then draw ellipse 


%% SAMPLE ALPHAS FROM LIKE A CERTAIN ELLIPSOID 
%% and then map back 

\subsection{Algorithm}

\begin{enumerate}
                         %need to fix this, because we CANNOT be on the edge to start with 
  \item{Begin with a point $x_0 \in K$. This starting point must be in the polytope.} % reference log barrier, explain in detail
  \item{Construct $D_{x_0}$, the Dikin Ellipsoid centered at $x_0$.}                  % that it blows up
  \item{Pick a random point $y$ from $D_{x_0}$.}           %%% (a) instead of step 1 and stuff
  \item{If $x_0 \notin D_{y}$, then reject $y$. Normally we would 
        think that $y$ must be contained in the Dikin ellipsoid of $x_0$ to be accepted. However, 
        this step actually says that if the current point $x_0$ is not in the Dikin Ellipsoid of the 
        potential point $y$, then we reject the point $y$.} 
                        % step 4 is necessary 
                        % explain why this exists
                        % just do the hessian
  \item{If $x_0 \in D_{y}$, then accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ 
        ( is that the ratio of the determinants are equal
         to the ratio of volumes of the ellipsoids centered at $x_0$ and $y$. Thus, 
         the geometric argument would be that %% why do we care about ratio 
         this way the Dikin walk can avoid extreme corners of the region)}
         
         %% somewhere mention r <= 1 
         
         The ratio of the determinants is equal .... ratio of $y$ and $x_0$. 
         
         2 cases: when volume of y is bigger than or equal to volume of x0. if thats case, we accept. 
         if however, y is smaller than x, we're ready to take it some of the time, but the further 
         away it is, the less likely we want to do it. 
         
         
         
         
         
  \item{Repeat until obtained number of desired points.} %% same sentence as hit-and-run

\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin2.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin3.png}
\label{fig:subfigure2}}
\quad    %% EXPLICITY CONSTRUCT D_y 
\subfigure[Step 3 Case I]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin41.png}
\label{fig:subfigure3}}
\subfigure[Step 3 Case II]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin42.png}
\label{fig:subfigure4}}
\quad
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin5.png}
\label{fig:subfigure5}}
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin6.png}
\label{fig:subfigure6}}
\caption{The Dikin Walk begins by constructing the Dikin Ellipsoid at the starting point $x_0$ (Step 1). 
         An uniformly random point $y$ is generated in the Dikin Ellipsoid centered at $x_0$ (Step 2). 
         If point $x_0$ is not in the Dikin Ellipsoid centered at $y$, then reject $y$ (Step 3 Case I).
         If point $x_0$ is contained in the Dikin Ellipsoid centered at $y$, then  %%% elaborate!!
         accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ (Step 3 Case II).
         Once we've successfully accepted $y$, we set $y$ as our new point, $x_1$ (Step 4). 
         Algorithm repeats (Step 5).} %% cannot begin with a point on the edge 
         
\label{fig:figure}
\end{figure}


%% need a few paragraphs here

% 1. Why is dikin better
% 2. Histograms == > SHOW A FIGURE, BUT NO R CODE ==> LABEL GRAPH  (A = matrix(1, ncol = 2))
% 2D line of how dikin is more concentrated than hitandrun

%% TALK ABOUT WALKR IMPLEMENTS DIKIN USING RCPP AND RCPPEIGEN
%% 

% 3. There are techniques to improve the sampling. a)thining b) burn-in, spend a paragraph each of those things 
% explain how a standard setting for burn-in is x (default settings for thin and burn-in) 
% LOOK AT STAN, WHAT REASONABLE VALUES ARE (cite the stan manual)

%% talk about how we know if we have enough thin and burn-in. Explain Rhat, gelman rubin and stuff. Give formula 
%% this indicates mixing. Cite the Gelman Rubin Article. 

\section{Using \texttt{walkr}}

The \pkg{walkr} package has one main function \code{walkr} which samples points. \code{walkr} has the following parameters:

\begin{itemize}

  \item{ \code{A} is the left hand side of the matrix equation $Ax=b$.}
  \item{ \code{b} is the right hand side of the matrix equation $Ax=b$.}
  \item{ \code{points} is the number of points you want.}
  \item{ \code{method} is the method of sampling: either \code{"hit-and-run"} or 
         \code{"dikin"}.}
  \item{ \code{thin} is the thinning parameter. Every \code{thin}\textsuperscript{th} 
          point is returned. Default is \code{1}.} % stan is 1
  \item{ \code{burn} is the burn-in parameter (as a percentage).  
        The first \code{burn} points are deleted from the final sample. Default is \code{0.5}, for 50\%.} % what's stan? 
  \item{ \code{chains} is the number of indepedent random walks we create, each
          from a different starting point. By default, walkr returns a matrix 
          which consists of the individual chains combined together. Every chain is an element 
          of the list which \code{walkr} eventually returns.}
  \item{ \code{ret.format} is the return format of the sampled points. If \code{"list"}, then
         a list of chains is returned, with each chain as a matrix of points. If \code{"matrix"}, 
         then a single matrix of points is returned. A column is a sampled point.}

\end{itemize}

Consider the 3D simplex: 

<<example1, eval = TRUE>>=
library(walkr)
A <- matrix(1, ncol = 3)
b <- 1
set.seed(314)
sampled_points <- walkr(A = A, b = b, points = 1000, 
                        method = "hit-and-run", chains = 5, ret.format = "matrix")
@

We can take a look at the sampled points. 

<<example2, eval = TRUE, fig.height = 5, fig.width = 5>>=
library(scatterplot3d)
scatterplot3d(x = sampled_points[1,], y = sampled_points[2,], z = sampled_points[3,], 
              scale.y = 0.5, xlab = "x1", ylab = "x2", zlab = "x3")
@

Sampling from higher dimensions follows the same syntax. Note that \code{walkr} automatically intersects 
$Ax=b$ with the $N$-simplex, so that the user does not have to include the simplex constraint in $Ax=b$. 
In this way, \pkg{walkr} is not a general tool for sampling from convex polytopes. Instead, 
it specializes in solving a special kind of convex polytopes, one created by the intersection of $Ax=b$
and the $N$-simplex. 

<<example3, eval = TRUE>>=
A <- matrix(sample(c(0,1,2), 40, replace = TRUE), ncol = 20)
b <- c(0.5, 0.3)
sampled_points <- walkr(A = A, b = b, points = 10000, chains = 5, 
                        method = "hit-and-run", ret.format = "list")

@

\code{walkr} warns the user if the chains have not mixed ``well-enough'' according to the 
$\hat{R}$ values. We can ensure better mixing by increasing the amount of thinning, and hence 
the number of total points sampled. 

<<example4, eval = TRUE>>=
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 500, 
                        method = "hit-and-run", ret.format = "list")         
@

Alternatively, we could use Dikin, which mixes better.

<<example5, eval = TRUE>>=
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 10,  
                        method = "dikin", ret.format = "list")
@

Dikin walk only required \code{thin} to be \code{10}. If we ran hit-and-run 
with a \code{thin} of 10, and even \code{250}, that code would've produced
a warning. This is a sign of Dikin mixing faster than hit-and-run. For higher
dimensions of $A$, Dikin requires much fewer points (or equivalently, lower \code{thin}) 
to pass the $\hat{R}$ test than hit-and-run. 

Now, \code{sampled\_points} contain 1000 sampled points. We can visualize the MCMC 
random walks by calling the \code{explore\_walkr} function, which launches a \code{shiny} interface 
from \pkg{shinystan}. Note that when calling \code{explore\_walkr}, the \code{"ret.format"} argument
from \code{walkr} must be \code{"list"}, so we know which points came from which chain. 

%Figure~\ref{fig:traceplot} shows the traceplots of a particular parameter.

\section{Conclusion}

The \pkg{walkr} package samples from the intersection of two spaces. 
The first space is all possible vectors $x$ that satisfy matrix equation $Ax=b$
($A$ is $M \times N$, with $M < N$), which describes $M$ unbounded hyperplanes
in $\mathbb{R}^N$. The second space is the $N$-simplex, defined as 
$x_1 + x_2 + x_3 + ... + x_N = 1$ and $x_i \geq 0$. This intersection of the 
two spaces is a non-negative convex polytope. 

\pkg{walkr} samples from a non-negative convex polytope using Monte-Carlo
Markov Chain (MCMC) algorithms. MCMC methods begin at a starting point
within the convex polytope and ``wanders'' through the polytope according 
to our algorithms. Every MCMC step depends only on the previous step and nothing
else. To examine the quality of the samples, we first create multiple random walks
from different starting points, which we call independent ``chains''. The ``mixing''
of different chains is one way of examining the quality of the samples. Good mixing
means that the different chains have thoroughly moved around the sample space and have 
overlapped with each other. Poor mixing means that the different chains have been stuck
at their initial values. Although good mixing does not guarantee an ideal sample, 
poor mixing suggests that more points must be sampled. 

The two MCMC algorithms implemented are hit-and-run and Dikin walk. Hit-and-run
guarantees an uniform sample asymptotically, but mixes more slowly with 
increasing columns of $A$. Dikin walk samples ``nearly'' uniformly, avoiding sampling
points near edges of the polytope, but mixes much faster. 

The major problem with our current implementaton is that run-time becomes unwiedly 
as the number of columns $N$ in $A$ increases. For lower dimensions of $A$ -- below $50$, 
hit-and-run and Dikin can both generate a well mixed sample within a few minutes. However,
for dimensions near $500$, it takes Dikin a few hours to generate a well mixed sample,
and it would take hit-and-run much longer. In applications, we recommend using 
Dikin walk instead of hit-and-run for values of $N$ greater than 50. Further research
is required for larger values of $N$. 

One possible extension to \pkg{walkr} is parallelization. Especially for Dikin, 
the majority of the run-time is spent on matrix multiplication and inversion. 
Since matrix multiplication could be parallized, the run-time issue in higher dimensions
could be solved when multiple cores are used at the same time. Another extension
would be to seek algorithmic improvements in rapid mixing, in the same way that Dikin walk is 
biased towards the center of the sample space.


\section{Authors}

\address{Andy Yao\\
Mathematics and Physics\\
Williams College \\
3123 Paresky Center\\
Williamstown, MA 01267\\
United States}
\email{andy.yao17@gmail.com}

\address{David Kane\\
Harvard University \\   
IQSS\\
1737 Cambridge Street\\
CGIS Knafel Building, Room 350\\
Cambridge, MA 02138\\
United States}
\email{dave.kane@gmail.com}

\bibliography{walkr}

\end{article}
\end{document}