\documentclass[a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage[round]{natbib}
\usepackage{subfigure}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{walkr}

\begin{document}

%% do not edit, for illustration only
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{YY}
\year{20ZZ}
\month{AAAA}

\begin{article}

\title{walkr: MCMC Sampling from Non-Negative Convex Polytopes}

\author{by Andy Yao, David Kane}

\maketitle      

\abstract{
Consider the intersection of two spaces: the complete solution space to $Ax=b$ and the $N$-simplex, 
described by $\sum\limits_{i=1}^N {x_i} = 1$ and $x_i \geq 0$. The intersection of 
these two spaces is a non-negative convex polytope. The \code{R} package \pkg{walkr} samples from this 
intersection using two Monte-Carlo Markov Chain (MCMC) methods: hit-and-run and Dikin walk. \pkg{walkr}
also provides tools to examine sample quality.
}

\section{Introduction} 

Consider all possible vectors $x$ that satisfy the matrix equation $Ax=b$,
where $A$ is $M \times N$, $x$ is $N \times 1$, and $b$ is $M \times 1$. 
The problem is interesting when there are more rows than columns  ($M < N$).
In general, if $M = N$, then there is a single solution, and if $M > N$, then there are no solutions.
If the rows of $A$ are linearly dependent, rows can be eliminated until they are linearly 
independent without changing the solution space. Therefore, we assume that the rows of $A$ are 
linearly independent going forward. 

Geometrically, every row in $Ax=b$ describes a hyperplane in $\mathbb{R}^N$. Therefore, $Ax=b$ represents
the intersection of $M$ unbounded hyperplanes. We bound the sample space by also 
requiring vector $x$ to be in the $N$-simplex, defined as: 

$$x_1 + x_2 + x_3 + ... + x_N = 1$$
$$x_i \geq 0, \qquad \forall \quad i \in \text{\{$1, 2, ..., N$\}}$$ 

The $N$-simplex is a $N-1$ dimensional object living in $N$ dimensional space. For example, the $3$D-simplex 
is a two dimensional equilateral triangle in three dimensional space (Figure~\ref{fig:simplex3D}).

\begin{figure}[H]
\centering
\includegraphics[width = 3.5in, height = 2.5in]{img/simplex3D.png}
\caption[caption]{The $3$D simplex is a two dimensional triangle in three dimensional space. The vertices
                  of the simplex are $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$. $x_1$, $x_2$, and $x_3$ 
                  are all greater than or equal to $0$, and for all points on the simplex, the sum
                  of $x_1$, $x_2$ and $x_3$ equals $1$.}
\label{fig:simplex3D}  
\end{figure} 

The intersection of the complete solution of $Ax=b$ and the $N$-simplex is a non-negative
convex polytope. Sampling from such an object is a difficult problem, and the common approach is to 
run Monte-Carlo Markov Chains (MCMC) (\cite{ravi}). MCMC methods begin at a starting point in
the sample space (in our case, a non-negative convex polytope), and randomly ``wanders'' through
the sample space according to an algorithm. An important feature of MCMC is that every step
depends only on the previous step and nothing else. 

MCMC sampling generally involve the creation
of multiple random walks from different starting points, each of which is an independent ``chain''.
A key aspect of running multiple chains from different starting points is to examine the 
``mixing'' of the sample. Good mixing means that the different chains 
(from different starting points) have overlapped with each 
other, hinting that they have thoroughly moved around the sample space. 
While good mixing does not guarantee a thorough sample of the space, poor mixing means 
unthorough sampling, so the random walks need to be run longer to cover more regions of the sample space. 
\pkg{walkr} allows the user to examine the quality of the MCMC samples.

\pkg{walkr} includes two MCMC algorithms: hit-and-run and Dikin walk.
Hit-and-run guarantees uniform sampling asympotically, but mixes more slowly
as the number of columns in $A$ increase (\cite{vempala}). Dikin walk generates a ``nearly'' uniform
sample --- favoring points away from the edges of the polytope --- but exhibits much faster mixing (\cite{kannan}). 

\section{Three dimensional example}

Consider one linear constraint in three dimensions.

$$x_1 + x_3 = 0.5$$

We can express this in terms of the matrix equation $Ax=b$:

$$
A = 
\begin{bmatrix}
1 & 0 & 1 \\
\end{bmatrix},
\quad
b = 0.5,
\quad
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
$$ 

Figure~\ref{fig:3dcase} shows the intersection of the $3$D-simplex with $Ax=b$. 

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/3Dcase.png}
\caption[caption]{The orange triangle is the $3$D-simplex. The blue plane is the hyperplane $
                  x_1+x_3=0.5$. The red line segment is their intersection, which is our sample space. 
                  The end points of the line segment are $(0.5,0.5,0)$ and $(0,0.5,0.5)$.}
\label{fig:3dcase} 
\end{figure} 

\section{Four dimensional example}

Just as the $3$D-simplex is a $2$D surface living in $3$D space, the $4$D-simplex 
(i.e. $x_1+x_2+x_3+x_4=1$, $x_i \geq 0$) can be viewed as a $3$D object, as in Figure~\ref{fig:tetra1}. 
Specifically, the $4$D-simplex is a tetradhedron when viewed from $3$D space, with 
verticies $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$.

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra1.png}
\caption[caption]{The 4D-simplex exists in 4D space, but can be viewed as a 3D object. Specifically, the
                  4D-simplex is a tetrahedron, with all four sides equilateral triangles.} 
\label{fig:tetra1}
\end{figure} 

Figure~\ref{fig:tetra2} shows the intersection of the $4$D-simplex with a
hyperplane (1 equation, or 1 row in $Ax=b$). The resulting convex polytope is a 2D trapezoid in $4$D space. 
Note that this convex polytope is $4-(1+1)=2$ dimensions. This is because we began with $4$ dimensions,
and the constraint and the simplex each reduced the dimension of the solution space by 1. 
In general, the resulting polytope will be $N-(M+1)$ dimensional.

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra2.png}
\caption[caption]{The 4D-simplex is a tetrahedron when projected to 3D space. 
                  The hyperplane $22x_1+2x_2+2x_3+37x_4=16$ cuts through the tetrahedron, forming 
                  a trapezoid as the intersection (in red). This trapezoid is our sample space, as 
                  it is the intersection of the hyperplane with the 4D-simplex. The vertices
                  of the trapezoid are ($0.7,0.3,0,0$), ($0.7,0,0.3,0$), ($0,0.6,0,0.4$),
                  and ($0,0,0.6,0.4$).} 
\label{fig:tetra2}
\end{figure}

In higher dimensions, the same logic applies. Each row in $Ax=b$ is a hyperplane living in $\mathbb{R}^N$.
Geometrically, our sample space is the intersection of $M$ hyperplanes with the $N$-simplex, 
which will be $N-(M+1)$ dimensional. 

\section{$x$-space and $\alpha$-space}

Our sample space is a bounded, non-negative convex polytope. In the literature, convex polytopes 
are commonly described by a generic $Ax \leq b$. In order to use the sampling algorithms, we must
first re-express the sample space in the form $Ax \leq b$ (with different $A$, $x$ and $b$)
\footnote{
This is a total abuse of notation. The $A$ in $Ax=b$ is very different from the $A$ in
$Ax \leq b$. $A$ is $N$ by $N-(M+1)$. $x$ and $b$ are also different.
The mathematical literature for linear equations uses $Ax = b$, and the litearture
on convex polytopes uses $Ax \leq b$, so it seemed best to use the same notation in both places 
in order to make the connections to existing litearture easier.
}. 

Recall that our sample space is the intersection of the complete solution to $Ax=b$ and 
the $N$-Simplex, which could be represented by three parts. First, the matrix equation 
$Ax=b$. Second, the simplex equation $x_1+x_2+...+x_N=1$. Third, the simplex 
non-negative inequalities, $x_i \geq 0$. In this section, we present a three step procedure
to combine all three parts into one single inequality of the form $Ax \leq b$. 

\subsection{Step 1: combining the simplex equality with the original $Ax=b$}

Recall that $A$ in $Ax=b$ is $M \times N$: 

$$
A = 
\overbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
  \end{bmatrix}
}^\text{N columns}
\Bigg\}\text{{M rows}}
$$

Add an extra row in $Ax=b$ which captures the equality part of the simplex constraint ($x_1+x_2+...+x_N=1$).
Call this new matrix $A'$:

$$
A' =
  \begin{bmatrix}
   & &  & & & \\
   & & A  & & & \\
   & &  & & & \\
   1 & 1 & ...... & 1 & 1 \\

  \end{bmatrix}, 
\quad 
b' = 
  \begin{bmatrix}
   \\
  b \\
   \\
  1 \\
  \end{bmatrix}
$$

\subsection{Step 2: solving for the null space -- transforming to $\alpha$-space}

Find $x$'s that satisfy $A'x = b'$. First, solve for the null space of $A'$, 
defined as all $x$ that satisfy $A'x=0$. The null space is 
spanned by $N-(M+1)$ basis vectors, because that is the dimension of the sample space
(our polytope). Any vector formed by 
a linear combination of the basis vectors will still be in the null space. 

Once we have the null space, we add on a particular solution to the null space. 
The null space we can think of as constructing a coordinate
system for $A'x=0$', and the particular solution to $A'x=b'$ we can think of as an offset from the origin 
to fit $A'x = b'$. For a review of the specifics of finding null spaces and particular solutions,
one can refer to \cite{leon}.

The null space of $A'$ can be represented by $N-(M+1)$ basis vectors. Because we are in $\mathbb{R}^N$, 
every basis vector, $v_i$, has $N$ components:

$$
\text{basis vectors} = 
\Bigg\{
v_1, \quad v_2, \quad v_3, \quad ...... \quad , \quad v_{N-(M+1)}
\Bigg\}
$$

Once we have the null space basis vectors and a particular solution, $v_{particular}$, 
we can express the set of all $x$'s that satisfy $A'x=b'$
in terms of coefficients $\alpha_i$. The complete
solution to $A'x=b'$ could be expressed as the set: 

$$
\Bigg\{ x = 
v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad | \quad \alpha_i \in \mathbb{R}
\Bigg\}
$$ 

\subsection{Step 3: including the simplex inequalities}

We add the inequality constraints from the $N$-simplex, requiring every element of vector $x$ to be 
greater equal to $0$: 

$$x = v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

We express all coefficients $\alpha_i$ as a vector $\alpha$:

$$\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
... \\
\alpha_{N-(M+1)}
\end{bmatrix}
$$

We can also express the set of basis vectors as columns of matrix $V$:  

$$
V = 
\begin{bmatrix}
v_1 &
v_2 &
... &
v_{N-(M+1)} \\
\end{bmatrix}
$$

Therefore, the inequality now becomes:

$$
v_{particular} + V\alpha \geq 
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

$$
V\alpha \geq -v_{particular}
$$

$$
-V\alpha \leq v_{particular}
$$

Finally, we arrive at the desired convex polytope form $Ax \leq b$, which is 
$-V\alpha \leq v_{particular}$ in our case.

\subsection{Four dimensional transformation example}

Consider a four dimensional example drawn from Figure~\ref{fig:tetra2}. 

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\textbf{Step 1:} Add an extra row in $Ax=b$ to capture the simplex equality.

$$
A' = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
1 & 1 & 1 & 1 \\
\end{bmatrix},
\quad
b' = 
\begin{bmatrix}
16 \\
1 \\
\end{bmatrix}
$$

\textbf{Step 2:} The null space basis contain 2 vectors, as $M-(N+1) = 4 - (1+1) = 2$ is the dimension of our solution space.
The null space basis vectors (to three decimal places) are:

$$
v_1 = 
\begin{bmatrix}
-0.103 \\
-0.680 \\
0.723 \\
0.059 \\
\end{bmatrix},
\quad
v_2 = 
\begin{bmatrix}
-0.833 \\
0.265 \\
0.092 \\
0.476
\end{bmatrix}
$$

A particular solution to $A'x=b'$ is (any particular solution works):

$$
v_{particular} = 
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
$$

\textbf{Step 3:} We add on the simplex inequalities:

$$
v_{particular} + \alpha_1 v_1 + \alpha_2 v_2 
\geq
\begin{bmatrix}
0 \\
0 \\
0 \\ 
0 \\
\end{bmatrix}
$$

Finally, we re-express the inequalities as $-V\alpha \leq v_{particular}$, which is of the form $Ax \leq b$

$$
V = 
\begin{bmatrix}
-0.103 & -0.833\\
-0.680 & 0.265\\
0.723 & 0.092\\
0.059 & 0.476\\
\end{bmatrix}
$$

$$
\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
$$

$$
\begin{bmatrix}
0.103 & 0.833\\
0.680 & -0.265\\
-0.723 & -0.092\\
-0.059 & -0.476\\
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
\leq
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
$$

$$
-V\alpha \leq
v_{particular}
$$

We sample $\alpha$'s according to our MCMC sampling algorithms. Then,
we map the $\alpha$'s back as $x$'s in the original coordinate system by:  
$$
x = 
v_{particular} + 
V\alpha
$$

For example, an $\alpha$ we sample can be:

$$
\alpha = 
\begin{bmatrix}
-0.149 \\
-0.372 \\
\end{bmatrix}
$$

Apply the map to $\alpha$, obtaining $x$ in the original problem statement. 

$$
x = 
v_{particular}+V\alpha
=
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
+
\begin{bmatrix}
-0.103 & -0.833\\
-0.680 & 0.265\\
0.723 & 0.092\\
0.059 & 0.476\\
\end{bmatrix}
\begin{bmatrix}
-0.149 \\
-0.372 \\
\end{bmatrix}
=
\begin{bmatrix}
0.539 \\
0.152 \\
0.219 \\
0.090 \\
\end{bmatrix}
$$

Indeed, the mapped point satisfies the original $Ax=b$ and the $N$-simplex equation and inequalities. 

Recall that we began with the intersection
of the complete solution to $Ax = b$ and the $N$-simplex, represented as three different parts which 
we dealt with individually. First, we add the $x_1+x_2+...+x_N=1$ part of the simplex equation 
as an extra row to $Ax=b$. Second, we solve for a particular solution 
and the the null space of $A$, obtaining the matrix $V$ which 
contains the basis vectors of $A$'s null space. Finally, we add the inequality constraints $x_i \geq 0$ 
to obtain $-V\alpha \leq v_{particular}$. 

Going forward, we will not use the $-V \alpha \leq v_{particular}$ notation and will instead
use the notation $Ax \leq b$, to denote the same matrix inequality. 
This is a total abuse of notation, as the $A$, $x$ and $b$ in $Ax \leq b$ are actually $-V$, 
$\alpha$ and $v_{particular}$. We do this because in the 
convex polytopes literature, it is standard to represent the polytope as $Ax \leq b$, so
it seemed best to use the same notation in order to make the connections to existing literature easier. 

\section{Algorithms}

\textbf{Important:} Before moving to sampling algorithms, we restate our problem.
Define non-negative convex polytope $K$ to be the solution to $Ax \leq b$
\footnote{
Again, the $Ax \leq b$ is different from the $Ax = b$ we began with.
For a detailed explaination on how we went from the intersection of $Ax = b$ and the $N$-simplex
to $Ax \leq b$, with different $A$, $x$, $b$, see the previous section. 
}. We are interested in sampling from $K$.

The two Monte-Carlo Markov Chain (MCMC) sampling methods we implement are hit-and-run
and Dikin walk. MCMC methods begin at a starting point in $K$ and wanders through $K$
according to our sampling algorithms. Every MCMC step depends only on the previous step and nothing else.
Ideally, we want to thoroughly sample the polytope $K$.  

To examine the quality of our sample, we create multiple, independent ``chains'' from 
different starting points in $K$ and observe their ``mixing''. The chains have mixed well 
if the parameters of different chains have overlapped with each other. If the chains have not mixed well, 
then we need to run the chains for longer (i.e. sample more points). While good mixing does 
not guarantee that the sample is perfect, poor mixing definitely points at a low
quality, unthorough sample.

\subsection{Starting point}

MCMC random walks need a starting point, $x_0$ in $K$. \pkg{walkr}
generates starting points using linear programming.
Specifically, the \texttt{lsei} function of the \pkg{limSolve} package (\cite{limsolve}) finds $x$ which: 

$$\text{minimizes} \quad |Cx-d|^2$$
$$\text{subject to} \quad Ax \leq b, \quad \text{this is our polytope $K$}$$

We randomly generate matrix $C$ and vector $d$. Solving this system generates an $x$ 
which will usually fall on the boundaries of polytope $K$. We repeat this process 30 times in 
\pkg{walkr} and take an average of those points, which generates one starting point $x_0$. 


\section{Hit-and-run} 

\cite{vempala} provides an overview of the hit-and-run algorithm, as follows:

\begin{enumerate}
  
  \item{Set starting point $x_0$ as current point.}
  \item{Generate a random direction $d$ from the $N$ dimensional unit-sphere.}
  \item{Find the chord $S$ through $x_0$ along the directions $d$ and $-d$. 
        Define end points $s_1$ and $s_2$ as the intersection of the chord $S$
        with the edges of $K$. Because $K$ is convex, the chord $S$ will only 
        intersect it at two points. Parametrize the chord $S$ by $s_1 + t(s_2-s_1)$, 
        where $t \in [0,1]$.}
  \item{Pick a random point $x_1$ along the chord $S$ by generating $t$ from \texttt{Uniform[0,1]}.}
  \item{Set $x_1$ as current point.}
  \item{Repeat algorithm until number of desired points sampled.}
        
\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_trapezoid1.png}
\label{fig:subfigure1}}
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun2.png}
\label{fig:subfigure1}}
\quad
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun3.png}
\label{fig:subfigure2}}
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun4.png}
\label{fig:subfigure3}}
\quad
\subfigure[ ]{
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun5.png}
\label{fig:subfigure4}}
%
\caption{(a) The hit-and-run algorithm begins with an interior point $x_0$. (b) A random direction  
         is selected. (c) The chord along that direction is calculated. (d)          
         Then, we pick a random point along that chord and move there as our new point.
         (e) The algorithm is repeated to sample many points.} 
\label{fig:hitandrun}
\end{figure}

\pkg{walkr} uses the \code{har} function from the \pkg{hitandrun} package(\cite{hitandrun})
to implement hit-and-run. The hit-and-run algorithm asymptotically generates an uniform
sample in the convex polytope $K$. However, the mixing of hit-and-run becomes slower 
with increasing dimensions of $K$. In practice, this slow mixing behavior of hit-and-run
is observed when we run multiple chains from different starting points. Instead of 
overlapping with each other, the individual chains stay at their initial
values. This behavior corresponds to ``getting stuck in corners'' of a ``skinny''
polytope. To solve this problem, we must let the individual chains run for longer(i.e. 
sample more points). However, as the dimensions of the polytope increase,
the number of points we need to sample quickly become infeasible.  

\section{Dikin walk}

A Dikin walk is the second of two MCMC methods implemented in the \pkg{walkr} package.
A Dikin walk begins from a random starting point within the convex polytope $K$ 
and then creates a Dikin ellipsoid centered at the current point. It then moves around the space
by sampling from such Dikin ellipsoids, whose shape and size are determined by both the 
current point and the shape of $K$. 

Unlike hit-and-run, the Dikin walk does not sample uniformly throughout $K$. Instead,
it is biased towards points that are way from the edges and corners of $K$ (\cite{kannan}). 
This bias allows the chains to mix more quickly than hit-and-run, and therefore, 
to work better in higher dimensions. 

For non-negative convex polytope $K$, defined as all $x$ for which $Ax \leq b$, define $a_i$ 
as the $i^\text{th}$ row of $A$. Define $x_i$ and $b_i$ as the $i^\text{th}$ element of $x$ and $b$ 
respectively. The dimensions of $A$ are $m = N$ by $n = N-(M+1)$, where $M$ and $N$ are the dimensions
of $A$ in $Ax=b$, the original statement of our problem. 

\noindent \textbf{Log Barrier Function $\phi$:} 

$$\phi(x) = \sum {- \log(b_i - a_i^Tx)}$$

The log-barrier function of $Ax \leq b$ measures how extreme or ``close-to-the-boundary'' a point 
$x \in K$ is, because the negative $\log$ function tends to infinity as its argument tends to zero.
Since $Ax \leq b$, for every row in $Ax \leq b$, $b_i > a_i^Tx$. Therefore, 
as $x$ approaches the boundary of $K$ (as $a_i^Tx$ approaches $b_i$), the value of $\phi$
approaches infinity. Hence, this is a barrier function, because the value of $\phi$ is 
infinity on the boundary of $K$, where $b_i = a_i^Tx$. 
It is also exactly because of this that the starting point cannot on the boundary of $K$, 
but must be in the interior of $K$. 

\noindent \textbf{Hessian of Log Barrier $H_x$:} 

$$H_x = \nabla ^2 \phi(x) = ...... = A^T D A, \quad \text{where:}$$
$$D = diag(\frac{1}{({b_i - a_i^Tx})^2})$$  

$H_x$ is a $n \times n$ linear operator. $D$ is a $m \times m$ diagonal matrix. 
The Hessian matrix ($H_x$) contains the second derivatives of the function $\phi(x)$ with respect 
to the vector $x$. The Hessian describes the shape of the local landscape at $x$.

\noindent \textbf{Dikin Ellipsoid $D_{x_0}^r$} \\

Define the Dikin Ellipsoid centered at $x_0$ with radius $r$ as: 

\begin{center}
$D_{x_0}^r \quad = \quad $\{$y \quad | \quad (y-x_0)^T H_{x_0}(y-x_0) \leq r^2$\} 
\end{center}

In this context, the Hessian $H_{x_0}$ at $x_0$ is used as a local norm, which we call the ``Hessian norm''.
The Dikin Ellipsoid with radius $1$ is the collection of all the points around 
$x_0$ whose difference with $x_0$ ($y-x_0$) is within the unit threshold (i.e. $r=1$)
with respect to the ``Hessian norm''. In the literature, this norm for vector $v$ is often denoted as 
$||v||_{x_0}$, meaning the ``Hessian norm'' of vector $v$ with respect to $x_0$. 

The closer the point $x_0$ is to the boundary of polytope $K$,
the larger the value of the ``Hessian norm'', and thus, the smaller the range of 
allowed points given an unit threshold (i.e. $r=1$), 
which leads to a smaller Dikin ellipsoid. On the other hand, the further 
the point $x_0$ is to the boundary of polytope $K$, the ``Hessian norm'' is smaller and therefore
the range for allowed points is larger -- a bigger Dikin ellipsoid. 

To see why, consider the single variable case. Recall that the log barrier function is of the form 
$-\log(z)$, where $z = a_i^Tx - b_i$. The Hessian is the generalized second derivative, and the second derivative of $-\log(z)$
is $\frac{1}{z^2}$. Therefore, for vector $v$, Hessian norm at point $x$ would be 
$||v||^2_{x} = \frac{v^2}{z^2}$. The closer $z$ is to zero (the closer $x$ is to the boundary), the larger the norm. 


\subsection{Algorithm}

\begin{enumerate}
                
  \item{Begin with a point $x_0 \in K$. This starting point must be in the polytope and 
        not on the edge of polytope $K$. If $x_0$ is on the boundary, then $a_ix_0=b_i$ 
        for some $i$,
        and consequently, the log-barrier and its Hessian would be infinity. 
        $x_0$ must be in the interior of $K$.} 
  \item{Construct $D_{x_0}$, the Dikin Ellipsoid centered at $x_0$.}                  
  \item{Pick a random point $y$ from $D_{x_0}$.}          
  \item{If $x_0 \notin D_{y}$, then reject $y$. In words, 
        if the current point $x_0$ is not in the Dikin Ellipsoid of the 
        potential point $y$, then we reject the point $y$.
        The purpose of this step is to avoid making a step that is too extreme.} 
  \item{If $x_0 \in D_{y}$, then accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$.
        $\sqrt{\frac{det(H_{y})}{det(H_{x_0})}}$ is equal to $\frac{\text{volume of Dikin ellipsoid centered at 
        } x_0 }{\text{volume of Dikin ellipsoid centered at } y}$. 
        Recall that the volume of the ellipsoid reflects how close to the boundary the center is. The closer the 
        center is to the boundary, the smaller its Dikin ellipsoid volume. This transition probability is crucial
        because it avoids the algorithm from concentrating in the ``central region'' of the polytope. 
        Because we already 
        know that $x_0 \in D_{y}$, the step from $x_0$ to $y$ is within the allowed threshold, or in other words,
        not too extreme. If the point $y$ is closer to the boundary than $x_0$ is 
        (the ratio of volumes is greater than 1), we prefer this, so we move there with probability $1$. 
        If $x_0$ is closer to the boundary than $y$ (ratio smaller than 1), then
        we accept $y$ depending on the ellipsoids' volumes.  % maybe add this as footnote
        \footnote{We need not worry about the accepted point $y$
        not in $K$, because when we set $r=1$, any Dikin ellipsoid centered at $x_0 \in K$ ($x_0$ not on the boundary)
        will be fully contained in $K$ (see \cite{kannan} section 2.1.4)}}
  \item{Repeat until obtained number of desired points.} 

\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin2.png}
\label{fig:subfigure1}}
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin3.png}
\label{fig:subfigure2}}
\quad % construct Dikin ellipsoid -- done
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin41.png}
\label{fig:subfigure3}}
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin42.png}
\label{fig:subfigure4}}
\quad
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin5.png}
\label{fig:subfigure5}}
\subfigure[ ]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin6.png}
\label{fig:subfigure6}}
\caption{(a) The Dikin Walk begins by constructing the Dikin Ellipsoid at the starting point $x_0$. 
         This point cannot be on the boundary of the polytope, otherwise the log-barrier and 
         its Hessian would both be infinity. 
         (b) An uniformly random point $y$ is generated in the Dikin Ellipsoid centered at $x_0$. 
         (c) If point $x_0$ is not in the Dikin Ellipsoid centered at $y$, then reject $y$. 
         (d) If point $x_0$ is contained in the Dikin Ellipsoid centered at $y$, then 
         accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$. 
         (e) Once we've successfully accepted $y$, we set $y$ as our new point, $x_1$.  
         (f) Algorithm repeats.} 
         
\label{fig:figure}
\end{figure}

\newpage

Dikin mixes much faster than hit-and-run does, especially in higher dimensions. This is because 
the mixing of Dikin is independent of the geometry of polytope $K$, whereas hit-and-run
mixes slower in ``skinny'' regions of $K$, tending to get stuck in corners (\cite{kannan}).
Dikin's quick mixing comes with a cost of non-uniform sampling, as it generates a 
``nearly uniform'' sample. Because tthe log-barrier function and Hessian prevents Dikin
walk from reaching points that are very close to the boundary of $K$, the Dikin walk is concentrated
at regions that are away from the boundary of $K$. For an illustration, consider the following example. 

We sample from the $2$D-simplex, which is a line segment ($x_1+x_2=1$, $x_i \geq 0$). As we can see
in the histograms, hit-and-run samples uniformly across the line segment, with 
values uniformly distributed amongst [$0,1$]. On the other hand, Dikin concentrates in 
sampling from regions far away from the edges. 

<<example0, eval = T, fig.width = 3, fig.height = 3, cache = TRUE>>=
library(walkr, ggplot2)
A <- matrix(1, ncol = 2)
b <- 1 
sample_har <- walkr(A = A, b = b, points = 10000, method = "hit-and-run")
sample_dikin <- walkr(A = A, b = b, points = 10000, method = "dikin")

qplot(sample_har[1,], xlab = "x1", main = "Hit-and-run in the 2D simplex")
qplot(sample_dikin[1,], xlab = "x1", main = "Dikin walk in the 2D simplex")

@

The \pkg{walkr} package uses \pkg{Rcpp} and \pkg{RcppEigen} (\cite{rcpp}, \cite{rcppeigen}) 
to implement Dikin walk for faster matrix multiplication, inversion, and determinant calculation,
because most of Dikin walk's run-time cost incurs in matrix operations. This improvement in 
run-time speed is especially important when sampling from $A$ with many columns, corresponding to polytopes in 
higher dimensions. In such cases, a larger number of points need to be sampled in order
to obtain a well mixed sample. 

To improve the mixing of a sample, there are in general two techniques. First, is to ``thin'' 
the sample -- every thin\textsuperscript{th} sample is stored. Thinning helps with the mixing of the sample because
as the dimensions of the polytope increases, the rate at which the individual parameters change, in 
general, becomes slower. We note that thinning is equivalent to sampling more points
in total. Second, is to add a ``burn-in'' parameter -- for example, a ``burn-in'' of 50\% means that 
the first 50\% of the total sample is deleted. This is an effective technique when the starting 
point is in a corner or narrow region in the polytope. We must give time for the random walk
to escape the corner and reach other parts of the sample space. 

To quantitatively examine the mixing, we can use the Gelman-Rubin diagnostic on multiple chains 
from diverse starting points (\cite{gelmanrubin}). The general idea is that we measure the variance within each chain
($W$) and the variance between the chains ($B$). If the variance between the chains is 
substantially larger than the variance within each chain, then the Gelman-Rubin diagnostic
($\hat{R}$) indicates that the mixing is not well enough and thus, the chains should run for longer iterations.

%% a good link --> http://www.people.fas.harvard.edu/~plam/teaching/
                  %% methods/convergence/convergence_print.pdf
%% another good link --> https://pymc-devs.github.io/pymc/modelchecking.html

\section{Using \texttt{walkr}}

The \pkg{walkr} package has one main function \code{walkr} which samples points. \code{walkr} has the following parameters:

\begin{itemize}

  \item{ \code{A} is the left hand side of the matrix equation $Ax=b$.}
  \item{ \code{b} is the right hand side of the matrix equation $Ax=b$.}
  \item{ \code{points} is the number of points returned. The total number of points sampled
        may be more than this because of thinning and burn-in.}
  \item{ \code{method} is the method of sampling: either \code{"hit-and-run"} or 
         \code{"dikin"}.}
  \item{ \code{thin} is the thinning parameter. Every \code{thin}\textsuperscript{th} 
          point is returned. Default is \code{1}.} % stan is 1 
  \item{ \code{burn} is the burn-in parameter (as a percentage).  
        The first \code{burn} points are deleted from the final sample. Default is \code{0.5}, for 50\%.}  
  \item{ \code{chains} is the number of indepedent random walks we create, each
          from a different starting point. By default, walkr returns a matrix 
          which consists of the individual chains combined together. Every column is a sampled point.}
  \item{ \code{ret.format} is the return format of the sampled points. If \code{"matrix"}, 
         then a single matrix of points is returned. If \code{"list"}, then
         a list of chains is returned, with each chain as a matrix of points. 
         Every column is a sampled point.}

\end{itemize}

Consider the 3D simplex: 

<<example1, eval = TRUE, cache = TRUE>>=
A <- matrix(1, ncol = 3)
b <- 1
set.seed(314)
sampled_points <- walkr(A = A, b = b, points = 1000, 
                        method = "hit-and-run", chains = 5, ret.format = "matrix")
@

We can take a look at the sampled points with a $3$D plot. 

<<example2, eval = TRUE, fig.height = 5, fig.width = 5, cache = TRUE>>=
library(scatterplot3d)
scatterplot3d(x = sampled_points[1,], y = sampled_points[2,], z = sampled_points[3,], 
              scale.y = 0.5, xlab = "x1", ylab = "x2", zlab = "x3")
@

Sampling from higher dimensions follows the same syntax. Note that \code{walkr} automatically intersects 
$Ax=b$ with the $N$-simplex, so that the user does not have to include the simplex constraint in $Ax=b$. 
In this way, \pkg{walkr} is not a general tool for sampling from convex polytopes. Instead, 
it specializes in solving a special kind of convex polytope -- one created by the intersection of $Ax=b$
and the $N$-simplex. 

<<example3, eval = TRUE, cache = TRUE>>=
A <- matrix(sample(c(0,1,2), 40, replace = TRUE), ncol = 20)
b <- c(0.5, 0.3)
sampled_points <- walkr(A = A, b = b, points = 10000, chains = 5, 
                        method = "hit-and-run", ret.format = "list")

@

\code{walkr} warns the user if the chains have not mixed ``well-enough'' according to the 
Gelman-Rubin $\hat{R}$ values. We can ensure better mixing by increasing the amount of thinning, and hence 
the number of total points sampled. 

<<example4, eval = TRUE, cache = TRUE>>=
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 500, 
                        method = "hit-and-run", ret.format = "list")         
@

Alternatively, we could use Dikin, which mixes better.

<<example5, eval = TRUE, cache = TRUE>>=
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 10,  
                        method = "dikin", ret.format = "list")
@

Dikin walk only required \code{thin} to be \code{10}. If we ran hit-and-run 
with a \code{thin} of \code{10}, \code{100}, or even \code{250}, that code would've produced
a warning. This is a sign of Dikin mixing faster than hit-and-run. For higher
dimensions of $A$, Dikin requires much fewer points (or equivalently, lower \code{thin}) 
to pass the $\hat{R}$ test than hit-and-run. 

Now, \code{sampled\_points} contain 1000 sampled points. We can visualize the MCMC 
random walks by calling the \code{explore\_walkr} function, which launches a \code{shiny} interface 
from \pkg{shinystan}. Note that when calling \code{explore\_walkr}, the \code{"ret.format"} argument
from \code{walkr} must be \code{"list"}, so we know which points came from which chain. 

%Figure~\ref{fig:traceplot} shows the traceplots of a particular parameter.

\section{Conclusion}

The \pkg{walkr} package samples from the intersection of two spaces. 
The first space is all possible vectors $x$ that satisfy matrix equation $Ax=b$
($A$ is $M \times N$, with $M < N$), which describes $M$ unbounded hyperplanes
in $\mathbb{R}^N$. The second space is the $N$-simplex, defined as 
$x_1 + x_2 + x_3 + ... + x_N = 1$ and $x_i \geq 0$. This intersection of the 
two spaces is a non-negative convex polytope. 

\pkg{walkr} samples from a non-negative convex polytope using Monte-Carlo
Markov Chain (MCMC) algorithms. MCMC methods begin at a starting point
within the polytope and ``wanders'' through the polytope.
Every MCMC step depends only on the previous step and nothing
else. To examine the quality of the samples, we first create multiple random walks
from different starting points, which we call independent ``chains''. The ``mixing''
of different chains is one way of examining the quality of the samples. Good mixing
means that the different chains have well 
overlapped with each other. Poor mixing means that the different chains have been stuck
at their initial values. Although good mixing does not guarantee an ideal sample, 
poor mixing suggests that more points must be sampled. %could be better

The two MCMC algorithms implemented are hit-and-run and Dikin walk. Hit-and-run
guarantees an uniform sample asymptotically, but mixes more slowly with 
increasing columns of $A$. Dikin walk samples ``nearly'' uniformly, avoiding 
points near edges of the polytope. However, Dikin mixes much faster than hit-and-run does. 

The major problem with our current implementaton is that run-time becomes unwiedly 
as the number of columns $N$ in $A$ increases. For lower dimensions of $A$ -- below $50$, 
hit-and-run and Dikin can both generate a well mixed sample within a few minutes. However,
for dimensions near $500$, it takes Dikin a few hours to generate a well mixed sample,
and it would take hit-and-run even longer. In applications, we recommend using 
Dikin walk instead of hit-and-run for values of $N$ greater than 50. Further research
is required for larger values of $N$. 

One possible extension to \pkg{walkr} is parallelization. Especially for Dikin, 
the majority of the run-time is spent on matrix multiplication and inversion. 
Since matrix multiplication could be parallelized, the run-time issue in higher dimensions
could be solved when multiple cores are used at the same time. Another extension
would be to seek algorithmic improvements in rapid mixing, in the same way that Dikin walk is 
biased towards the center of the sample space.

\section{Authors}

\address{Andy Yao\\
Mathematics and Physics\\
Williams College \\
3123 Paresky Center\\
Williamstown, MA 01267\\
United States} \\
\email{andy.yao17@gmail.com}

\address{David Kane\\
Harvard University \\   
IQSS\\
1737 Cambridge Street\\
CGIS Knafel Building, Room 350\\
Cambridge, MA 02138\\
United States} \\
\email{dave.kane@gmail.com}

\bibliography{walkr}

\end{article}
\end{document}