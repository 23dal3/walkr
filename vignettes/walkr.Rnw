\documentclass[a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage[round]{natbib}
\usepackage{subfigure}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{walkr}

\begin{document}

%% do not edit, for illustration only
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{YY}
\year{20ZZ}
\month{AAAA}

\begin{article}

\title{walkr: MCMC Sampling from Non-Negative Convex Polytopes}

\author{by Andy Yao, David Kane}

\maketitle      

\abstract{
Consider the intersection of two spaces: the complete solution space to $Ax=b$ and the $N$-Simplex, 
described by $\sum\limits_{i=1}^N {x_i} = 1$ and $x_i \geq 0$. The intersection of 
these two spaces is a non-negative convex polytope. The \code{R} package \pkg{walkr} samples from this 
intersection using  %%% SPECIAL FORMAT FOR R 
two Monte-Carlo Markov Chain (MCMC) methods: hit-and-run and Dikin walk. \pkg{walkr}
also provide tools to examine sample quality.
}

\section{Introduction} 

Consider all possible vectors $x$ that satisfy the matrix equation $Ax=b$,
where $A$ is $M \times N$, $x$ is $N \times 1$, and $b$ is $M \times 1$. 
The problem is interesting when there are more rows than columns  ($M < N$). 
If $M = N$, then there is a single solution, and if $M > N$, there then are, in general, no solutions.
If the rows of $A$ are linearly dependent, rows can be eliminated until they are linearly 
independent without affect the solution space. Therefore, we can safely assume that the rows are linearly independent
going forward.

Geometrically, every row in $Ax=b$ describes a hyperplane in $\mathbb{R}^N$. Therefore, $Ax=b$ represents
the intersection of $M$ unbounded hyperplanes in $\mathbb{R}^N$. We bound the sample space by also 
requiring vector $x$ to be in the $N$-simplex. The $\mathbf{N}$\textbf{-simplex}: 

$$x_1 + x_2 + x_3 + ... + x_N = 1$$
$$x_i \geq 0, \qquad \forall i \in \text{\{$1, 2, ..., N$\}}$$ 

The $N$-simplex is a $N-1$ dimensional object living in $N$ dimensional space. For example, the $3$D simplex 
is a $2$ dimensional triangle in $3$D space (Figure~\ref{fig:simplex3D}).

\begin{figure}[H]
\centering
\includegraphics[width = 3.5in, height = 2.5in]{img/simplex3D.png}
\caption[caption]{The $3$D simplex is a $2$ dimensional triangle in $3$ dimensional space. The vertices
                  of the simplex are $(1,0,0)$, $(0,1,0)$, and $(0,0,1)$.}
\label{fig:simplex3D} 
\end{figure} 

The intersection of the complete solution of $Ax=b$ and the $N$-simplex is a non-negative
convex polytope. Sampling from such a convex polytope is a difficult problem, and the common approach is to 
run Monte-Carlo Markov Chains (MCMC) in the polytope (\cite{ravi}). 
\pkg{walkr} includes two MCMC algorithms: hit-and-run and Dikin walk.
Hit-and-run guarantees uniform sampling asympotically, but mixes increasingly slowly 
for higher dimensions of $A$ (\cite{vempala}). Dikin walk generates a ``nearly'' uniform
sample --- favoring points away from the edges of the polytope --- but exhibits much faster mixing (\cite{kannan}). 

MCMC methods generally involve the creation of multiple random walks from different starting points,
each of which is an indepedent ``chain''. To check for the quality of the samples, one could 
examine the distribution of the random walks at different starting points. \pkg{walkr} allows the user
to examine the quality of the samples. %% should edit, maybe mention shinystan

Once we have multiple chains, we could examine whether the chains have
all converged to the same stationary distribution. The correlation between different chains
is also of interest. Ideally, we hope to see a low correlation which reflects that
the chains are mixing well with each other.

\section{3 Dimensional Example}

Consider one linear constraint in three dimensions.

$$x_1 + x_3 = 0.5$$

We can express this in terms of the matrix equation $Ax=b$:

$$
A = 
\begin{bmatrix}
1 & 0 & 1 \\
\end{bmatrix},
\quad
b = 0.5,
\quad
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
$$ 

Figure~\ref{fig:3dcase} shows the intersection of the $3$D simplex with $Ax=b$. 

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/3Dcase.png}
\caption[caption]{The orange triangle is the $3$D-simplex. The blue plane is the hyperplane $
                  x_1+x_3=0.5$. The red line segment is their intersection, which is our sample space. 
                  The end points of the line segment are $(0,0.5,0.5)$ and $(0.5,0.5,0)$.}
\label{fig:3dcase} 
\end{figure} 

\section{4 Dimensional Example}

Just as the $3$D simplex is a $2$D surface living in $3$D space, the $4$D simplex 
(i.e. $x_1+x_2+x_3+x_4=1$, $x_i \geq 0$) can be viewed as a $3$D object, as in Figure~\ref{fig:tetra1}. 
Specifically, the $4$D simplex is the following tetradhedron when viewed from $3$D space, with 
verticies $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$.

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra1.png}
\caption[caption]{The 4D simplex exists in 4D space, but can be viewed as a 3D object. Specifically, the
                  4D simplex is a tetrahedron, with all four sides equilateral triangles.} 
\label{fig:tetra1}
\end{figure} 

Now imagine the intersection of the $4$D simplex with one hyperplane in $4$D (1 equation,
or 1 row in $Ax=b$). For a specific $A$ and $b$, we demonstrate the intersection 
in Figure~\ref{fig:tetra2}. The resulting shape is a trapezoid in $4$D space. 

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\begin{figure}[H]
\centering
\includegraphics[width = 3in, height = 2.5in]{img/tetra2.png}
\caption[caption]{The 4D simplex is the tetrahedron. The hyperplane $22x_1+2x_2+2x_3+37x_4=16$ cuts through 
                  the tetrahedron, forming 
                  a trapezoid as the intersection (in red). This trapezoid is our sample space, as 
                  it is the intersection of the hyperplane with the 4D simplex. The vertices
                  of the trapezoid are ($0.7,0.3,0,0$), ($0.7,0,0.3,0$), ($0,0.6,0,0.4$),
                  and ($0,0,0.6,0.4$).} 
\label{fig:tetra2}
\end{figure}

In higher dimensions, the same logic applies. Each row in $Ax=b$ is a hyperplane living in $\mathbb{R}^N$ (given $N$
variables). Thus, geometrically, our sample space is: \textbf{the intersection of $M$ hyperplanes
with the $N$-simplex}. 

\section{From $x$-space to $\alpha$-space}

Our sample space is a bounded, non-negative convex polytope. In the literature, convex polytopes 
are commonly described by a generic $Ax \leq b$. In order to use the sampling algorithms, we must
first re-express the sample space in the form $Ax \leq b$ (with different $A$, $x$ and $b$). Therefore,
in this section, we present
a 3 step procedure which transforms the intersection of $Ax=b$ and the $N$-simplex 
into the $Ax \leq b$ form (\textbf{note}: the $A$ and $b$ in $Ax \leq b$ is 
not the same as those which we began with in $Ax=b$. We use $A$ and $b$ 
in both occasions because it is standard notation for describing both a matrix equation and a convex polytope.)

\subsection{Step 1: Combining Simplex Equality with the Original $Ax=b$}

Recall that $A$ in $Ax=b$ is $M \times N$: 

$$
A_{M \times N} = 
\overbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
  \end{bmatrix}
}^\text{N columns}
\Bigg\}\text{{M rows}}
$$

Add an extra row in $Ax=b$ which captures the equality part of the simplex constraint ($x_1+x_2+...+x_N=1$).
Call this new matrix $A'$:

$$
A' =
  \begin{bmatrix}
   & &  & & & \\
   & & A  & & & \\
   & &  & & & \\
   1 & 1 & ...... & 1 & 1 \\

  \end{bmatrix}, 
\quad 
b' = 
  \begin{bmatrix}
   \\
  b \\
   \\
  1 \\
  \end{bmatrix}
$$

\subsection{Step 2: Solving for the Null Space -- Transforming to $\alpha$-space}

Second, we find all possible $x$'s that satisfy $A'x = b'$. To do so, we must first 
compute the null space of $A'$ and then add on any particular solution that satisfy
$A'x=b'$. See Leon's Linear Algebra textbook for a review of 
the specifics of finding null spaces and particular solutions(\cite{leon}).

The Null Space of $A'$ can be represented by $N-(M+1)$ basis vectors, since we have
$N$ variables and $M+1$ constraints in $A'$. Every basis vector, $v_i$, has 
$N$ components:

$$
\text{basis vectors} = 
\Bigg\{
v_1, \quad v_2, \quad v_3, \quad ...... \quad , \quad v_{N-(M+1)}
\Bigg\}
$$

Once we have the basis vectors, we could express the set of all $x$'s that satisfy $A'x=b'$
in terms of coefficients $\alpha_i$. The intuition would be that the basis vectors
form a coordinate system in the complete solution space, and that the coefficients $\alpha_i$'s
represent linear combinations of basis vectors to cover the whole space. The complete
solution to $A'x=b'$ could be expressed as the set: 

$$
\Bigg\{ x = 
v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad | \quad \alpha_i \in \mathbb{R}
\Bigg\}
$$ 

\subsection{Step 3: Including the Simplex Inequalities}

We add the inequality constraints from the $N$-simplex, requiring every element of vector $x$ to be $\geq 0$: 

$$x = v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

We express all coefficients $\alpha_i$ as a vector $\alpha$:

$$\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
... \\
\alpha_{N-(M+1)}
\end{bmatrix}
$$

We can also express the set of basis vectors as columns of matrix $V$:  

$$
V = 
\begin{bmatrix}
v_1 &
v_2 &
... &
v_{N-(M+1)} \\
\end{bmatrix}
$$

Therefore, the inequality now becomes:

$$
v_{particular} + V\alpha \geq 
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

$$
V\alpha \geq -v_{particular}
$$

$$
-V\alpha \leq v_{particular}
$$

Here we arrive at the generic $Ax \leq b$ representation of a convex polytope (note again
that the $A$ and $b$ in $Ax \leq b$ are different from those in $Ax=b$). We have performed
a \textbf{transformation} from $x$-space to $\alpha$-space, going from describing the polytope 
in terms of the intersection of $Ax=b$ and the $N$-simplex to the general $Ax \leq b$ form.
This is necessary because the following sample algorithms we are going to present rely on the fact that the 
polytope is described in the $Ax \leq b$ form. Therefore, \pkg{walkr} internally performs 
this transformation, samples $\alpha$, then maps it back to $x$-space.


\subsection{4 Dimensional Transformation Example}

To fix ideas, let's go through a 4 dimensional example. Let $A$ and $b$ be defined as:

$$
A = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16
\end{bmatrix}
$$

\textbf{Step 1:} add an extra row in $Ax=b$ to capture the equality part of the simplex constraint.

$$
A' = 
\begin{bmatrix}
22 & 2 & 2 & 37\\
1 & 1 & 1 & 1 \\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
16 \\
1 \\
\end{bmatrix}
$$

\textbf{Step 2:} the null space basis contain 2 vectors, as $M-(N+1) = 4 - (1+1) = 2$ is the dimension of our solution space.
The null space basis vectors (to three decimal places) are:

$$
v_1 = 
\begin{bmatrix}
-0.103 \\
-0.680 \\
0.723 \\
0.059 \\
\end{bmatrix},
\quad
v_2 = 
\begin{bmatrix}
-0.833 \\
0.265 \\
0.092 \\
0.476
\end{bmatrix}
$$

A particular solution to $A'x=b'$ is (any particular solution would work):

$$
v_{particular} = 
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
$$

\textbf{Step 3:} we add on the simplex inequalities:

$$
v_{particular} + \alpha_1 v_1 + \alpha_2 v_2 
\geq
\begin{bmatrix}
0 \\
0 \\
0 \\ 
0 \\
\end{bmatrix}
$$

Finally, we re-express the inequalities as $-V\alpha \leq v_{particular}$, which is of the form $Ax \leq b$

$$
V = 
\begin{bmatrix}
-0.103 & -0.833\\
-0.680 & 0.265\\
0.723 & 0.092\\
0.059 & 0.476\\
\end{bmatrix}
$$

$$
\alpha =
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
$$

$$-V\alpha = 
\begin{bmatrix}
0.103 & 0.833\\
0.680 & -0.265\\
-0.723 & -0.092\\
-0.059 & -0.476\\
\end{bmatrix}
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
\end{bmatrix}
\leq
\begin{bmatrix}
0.212 \\
0.147 \\
0.359 \\
0.274 \\
\end{bmatrix}
= v_{particular}
$$

\section{Algorithms}

\textbf{Important:} Before moving to sampling algorithms, we clear up on some nomenclature. 
First, as established above, the sampling space is a convex polytope. We shall refer to this convex 
polytope as $K$. Moreover, because it is standard notation in the literature to 
describe a convex polytope as $Ax \leq b$, we describe $K$ with $Ax \leq b$. 
In fact, the $Ax \leq b$ form is necessary to perform the sampling algorithms. We keep in mind that
the $A$, $x$, and $b$ are not the same as those in the original problem statement of $Ax=b$ and $N$-simplex. 
Instead, as described in the section above, we actually sample coefficients $\alpha$'s, 
eventually transforming $\alpha$'s back to $x$-space. Again, $Ax \leq b$ in this section describes the 
polytope in terms of transformed coordinates $\alpha$.

\subsection{Picking Starting Point}

MCMC random walks need a starting point, $x_0$, in the convex polytope. \pkg{walkr}
generates such starting points using linear programming.
Specifically, the \texttt{lsei} function of the \pkg{limSolve} package (\cite{limsolve}) finds $x$ which: 

$$\text{minimizes} \quad |Cx-d|^2$$
$$\text{subject to} \quad Ax \leq b, \quad \text{this is our polytope $K$}$$

We randomly generate $C$ and $d$, obtaining points that minize the random objective function. 
Because we are performing a minimization, the points obtained this way often fall
onto the boundaries of polytope $K$. Therefore, we repeat the process 30 times in \pkg{walkr}
and take an average of those points, which ought to be still in the 
polytope $K$ because it is convex. This procedure generates one starting point $x_0$. \\

Often times in MCMC random walks, it is beneficial to have multiple starting points at 
different locations in $K$. We call each random walk at a starting point a single chain.
For example, if we have 5 different starting points, each running its own random walk, 
we would have 5 chains. 

\section{Hit-and-run}

We point out that the $Ax \leq b$ used below is the transformed polytope
in $\alpha$-space (discussed in the section above). The $A$, $x$, and $b$ are different from
those in the original $Ax=b$ and $N$-Simplex. 

\begin{enumerate}
  
  \item{Set starting point $x_0$ as current point}
  \item{Randomly generate a direction $\vec{d}$. If we are in $N$ dimensions, then $d$ will 
        be a vector of $N$ components. Specifically, $d$ is a uniformly generated 
        unit vector on the $N$ dimensional unit-sphere}
  \item{Find the chord $S$ through $x_0$ along the directions $\vec{d}$ and $-\vec{d}$.  
        We find end points $s_1$ and $s_2$ of the chord by going through the rows 
        of $Ax_0 \leq b$ one by one, setting the inequality to equality (so we hit the surface). 
        Then, parametrize the chord along $x_0$ by $s_1 + t(s_2-s_1)$, where $t \in [0,1]$}
  \item{Pick a random point $x_1$ along the chord $S$ by generating $t$ from \texttt{Uniform[0,1]}}
  \item{Set $x_1$ as current point}
  \item{Repeat algorithm until number of desired points sampled}
        
\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_trapezoid1.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun2.png}
\label{fig:subfigure1}}
\quad
\subfigure[Step 3]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun3.png}
\label{fig:subfigure2}}
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun4.png}
\label{fig:subfigure3}}
\quad
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun5.png}
\label{fig:subfigure4}}
%
\caption{The hit-and-run algorithm begins with an interior point $x_0$ (Step 1). A random direction
         is selected (Step 2), and the chord along that direction is calculated (Step 3).
         Then, we pick a random point along that chord and move there as our new point (Step 4).
         The algorithm is repeated to sample many points (Step 5).}
\label{fig:hitandrun}
\end{figure}

\pkg{walkr} uses the \code{har} function from the \pkg{hitandrun} package on CRAN (\cite{hitandrun}) 
to implement hit-and-run. The hit-and-run algorithm asymptotically generates an uniform sample in the convex polytope. The cost
of each step for hit-and-run is also relatively inexpensive. However, as the dimensions of the polytope
increases, the mixing of hit-and-run becomes increasingly slower. 

\section{Dikin Walk}

Dikin walk is another method of MCMC random walks. In higher dimensions, the Dikin walk mixes much stronger
than hit-and-run does. This desirable mixing effect is due to the fact that Dikin walk favors points
that are far away from the edges of the polytope, at the cost of sampling a ``nearly uniform'' sample (\cite{kannan}).

\subsection{Definitions}

Recall, our sampling space is a convex polytope. We call this convex 
polytope $K$, which can be described in the form $Ax \leq b$. We point out that this $Ax \leq b$ 
is the transformed polytope in $\alpha$-space (discussed in transformation section above). 
The $A$, $x$, and $b$ are different from those in the original $Ax=b$ and $N$-Simplex.

For the definitions below, let $a_i$ represent a row in $A$. Let $x_i$, $b_i$ represent 
the $i^{th}$ element of $x$ and $b$ respectively. Let $A$ (the one in $Ax \leq b$) be 
of size $m \times n$ (not the same as $M \times N$).

\noindent \textbf{Log Barrier Function $\phi$:} 

$$\phi(x) = \sum {- \log(b_i - a_i^Tx)}$$

The log-barrier function of $Ax \leq b$ measures how extreme or ``close-to-the-boundary'' a point 
$x \in K$ is, because the negative $\log$ function tends to infinity as its argument tends to zero. 
The value of $\phi$ gets larger and larger as $a_i^Tx$ gets closer and closer to $b_i$. 

\noindent \textbf{Hessian of Log Barrier $H_x$:} 

$$H_x = \nabla ^2 \phi(x) = ...... = A^T D^2 A \quad , \quad \text{where:}$$
$$D = diag(\frac{1}{b_i - a_i^Tx})$$

\textbf{Note:} $H_x$ is a $n \times n$ linear operator. $D$ is a $m \times m$ diagonal matrix.\\

The Hessian matrix($H_x = \nabla ^2 \phi(x)$) contains all the second derivatives 
of the function $\phi(x)$ with respect to vector $x$. In the land of optimization, 
the Hessian contains information on how the landscape is shaped, and also on extreme values of 
the landscape. To develop a better intuition for the Hessian, we 
can view it as a generalized notion of the second derivative. The second derivative of $-\log(z)$ 
is $\frac{1}{z^2}$, which also tends to infinity as $z$ tends to zero. 

\noindent \textbf{Dikin Ellipsoid $D_{x_0}^r$} \\

Define $D_{x_0}^r$, the Dikin Ellipsoid centered at $x_0$ with radius $r$ as: 

\begin{center}
$D_{x_0}^r \quad = \quad $\{$y \quad | \quad (y-x_0)^T H_{x_0}(y-x_0) \leq r^2$\} 
\end{center}

The Dikin Ellipsoid with radius $r = 1$ is the unit ball centered at $x_0$ with respect to the 
"Hessian norm" as the notion of length. The "Hessian norm" we could think as a $\Delta \frac{1}{z^2}$ on 
the $\frac{1}{z^2}$ graph. For $x_0$'s that are far away from the boundary, given
an allowed $\Delta \frac{1}{z^2}$ (captured in $r$), the range of allowed values of $y-x_0$ is very large.
This corresponds to having a large Dikin ellipsoid. Alternatively, if $x_0$ is near the boundary,
then given an allowed $\Delta \frac{1}{z^2}$, only a small range of $y-x_0$ is acceptable -- a smaller
Dikin ellipsoid.

To rephrase, the Dikin Ellipsoid is the collection of all points whose difference with 
the current point($y-x_0$) is within the unit threshhold of $r=1$. When the center $x_0$ of the Dikin Ellipsoid 
is far from the boundary of the polytope, the ellipsoid is larger. As the center point $x_0$ approaches the boundary 
of the polytope, the ellipsoid becomes smaller and smaller. Therefore, the ellipsoid is able to reshape 
itself as it surveys through the polytope $K$, biasing away from the corners of the region.

\subsection{Algorithm}

\begin{enumerate}

  \item{Begin with a point $x_0 \in K$. This starting point must be in the polytope.}
  \item{Construct $D_{x_0}$, the Dikin Ellipsoid centered at $x_0$.}
  \item{Pick a random point $y$ from $D_{x_0}$.}
  \item{If $x_0 \notin D_{y}$, then reject $y$. Normally we would 
        think that $y$ must be contained in the Dikin ellipsoid of $x_0$ to be accepted. However, 
        this step actually says that if the current point $x_0$ is not in the Dikin Ellipsoid of the 
        potential point $y$, then we reject the point $y$.} %% I don't know why the other way around doesn't work...
                                                            %% probably some stats thing related to bias
  \item{If $x_0 \in D_{y}$, then accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ 
        (the big picture is that the ratio of the determinants are equal
         to the ratio of volumes of the ellipsoids centered at $x_0$ and $y$. Thus, 
         the geometric argument would be that 
         this way the Dikin walk can avoid extreme corners of the region). For $r \leq 1$, the Dikin algorithm
         will never leave the polytope $K$ (\cite{kannan}). Therefore, we need not be conerend that an accepted 
         $y$ will be outside our polytope $K$.}
  \item{repeat until obtained number of desired points.}

\end{enumerate}

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin2.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin3.png}
\label{fig:subfigure2}}
\quad
\subfigure[Step 3 Case I]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin41.png}
\label{fig:subfigure3}}
\subfigure[Step 3 Case II]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin42.png}
\label{fig:subfigure4}}
\quad
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin5.png}
\label{fig:subfigure5}}
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin6.png}
\label{fig:subfigure6}}
\caption{The Dikin Walk begins by constructing the Dikin Ellipsoid at the starting point $x_0$ (Step 1). 
         An uniformly random point $y$ is generated in the Dikin Ellipsoid centered at $x_0$ (Step 2). 
         If point $x_0$ is not in the Dikin Ellipsoid centered at $y$, then reject $y$ (Step 3 Case I).
         If point $x_0$ is contained in the Dikin Ellipsoid centered at $y$, then 
         accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ (Step 3 Case II).
         Once we've successfully accepted $y$, we set $y$ as our new point, $x_1$ (Step 4). 
         Algorithm repeats (Step 5).}
         
\label{fig:figure}
\end{figure}

\section{Using \texttt{walkr}}

%% this is helpful : http://yihui.name/knitr/options/

The \pkg{walkr} package has one main function \code{walkr} which samples points. \code{walkr} has the following parameters:

\begin{itemize}

  \item{ \code{A} is the right hand side of the matrix equation $Ax=b$.}
  \item{ \code{b} is the left hand side of the matrix equation $Ax=b$.}
  \item{ \code{method} is the method of sampling -- can be either \code{"hit-and-"run"} or 
         \code{"dikin"}.}
  \item{ \code{thin} is the thinning parameter. Every \code{thin}-th point is stored into the final sample.}
  \item{ \code{burn} is the burning parameter. The first \code{burn} points are deleted from the final sample.}
  \item{ \code{chains} is the number of chains we want to sample. Every chain is an element 
        of the list which \code{walkr} eventually returns.}
  \item{ \code{ret.format} is the return format of the sampled points. If \code{"list"}, then
         a list of chains is returned, with each chain as a matrix of points. If \code{"matrix"}, 
         then a single matrix of points is returned. A column is a sampled point. If the user 
         wants to visualize sampling with \code{explore\_walkr}, then she should return a list format.}

\end{itemize}

\subsection{Simple 3D Simplex}

To sample from the 3D simplex, the user can simply specify the simplex 
equation in $Ax=b$, as \code{walkr} will remove linearly dependent rows internally.

<<example1, eval = TRUE>>==
library(walkr)
A <- matrix(1, ncol = 3)
b <- 1
sampled_points <- walkr(A = A, b = b, points = 1000, 
                        method = "hit-and-run", chains = 5, ret.format = "matrix")
@

\subsection{Higher Dimensions with Constraints}

Sampling from higher dimensions follows the same syntax. The user just has to specify an $A$ and $b$.
Note that \code{walkr} automatically intersects $Ax=b$ with the $N$-Simplex, so that the user 
does not have to include the simplex equation in $Ax=b$. 

<<example3, eval = TRUE>>=
## two 20 dimensional constraints

set.seed(314)
A <- matrix(sample(c(0,1,2), 40, replace = TRUE), ncol = 20)
b <- c(0.5, 0.3)
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, 
                        method = "hit-and-run", ret.format = "list")


@

As we could see from the warning message above, \code{walkr} internally warns the user if 
the chains have not mixed "well-enough" or have not converged to a stationary distribution 
according to the Gelman-Rubin Diagnostics (\cite{gelman}). Gelman-Rubin suggests that if 
any of the parameter's $\hat{R}$ value is above $1.1$, then the chains should run longer,
or equivalently, we could increase the \code{thin} parameter. 

<<example4, eval = TRUE>>=
set.seed(314)
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 100, 
                        method = "hit-and-run", ret.format = "list")
sampled_points <- walkr(A = A, b = b, points = 1000, chains = 5, thin = 10, 
                        method = "dikin", ret.format = "list")

@

As we could see from the code chunk above, Dikin walk only required \code{thin} to be $10$, whereas 
hit-and-run needed a \code{thin} parameter of $100$. This is a sign that Dikin mixes faster 
than hit-and-run does. As dimensions ramp up to the hundreds, this rapid mixing behavior 
of Dikin compared to hit-and-run is even more obvious.

Now, \code{sampled\_points} contain 1000 sampled points. We can visualize the MCMC 
random walks by calling the \code{explore\_walkr} function, which launches a \code{shiny} interface 
from \pkg{shinystan}. Note that the \code{"ret.format"} argument previously must be \code{"list"}.
Figure~\ref{fig:traceplot} shows the traceplots of a particular parameter.

<<example2, eval = FALSE>>=
explore_walkr(sampled_points)
@

\begin{figure}[H]
\centering
\includegraphics[width = 6in, height = 2.5in]{img/traceplot.png}
\caption[caption]{The traceplots of 5 chains for points sampled from the intersection of the
                  20D simplex with two hyperplanes. Sampling algorithm used was the Dikin walk.} 
\label{fig:traceplot}
\end{figure}

\section{Dikin versus Hitandrun}

Let $Ax \leq b$ be the inequalities describing the transformed polytope. Let the dimensions of $A$ 
be $m \times n$ (not the same as $M \times N$ in the original $A$ for $Ax = b$).

\begin{tabular}{ |p{3cm}|p{5cm}|p{5cm}|}
  \hline
   &  \texttt{hit-and-run} & \texttt{Dikin Walk} \\ \hline
  Uniform Sampling & Yes, needs $O(n^3)$ points & 
  No, concentrates in the    interior\\ \hline
  Mixing & $O(\frac{n^2R^2}{r^2})$ *, slows down substantially as dimension of polytope increases and 
  polytope becomes "skinnier" & $O(mn)$; much stronger mixing. \\ \hline
  Cost of One Step & $O(mn)$ & $O(mn^2)$, in practice, one step of Dikin is much more 
  costly than hit-and-run\\ \hline
  Rejection Sampling & No & Yes (see probability formula and $x \notin D_y$), but rejection rate 
  not high\\ \hline

\end{tabular} \\ 

*$R$ is the radius of the smallest ball that contains the polytope $K$. $r$ is the radius of the 
largest ball that is contained within the polytope $K$. Thus, $\frac{R}{r}$ increases 
as the polytope is "skinnier"(\cite{kannan}). \\

As we can in the two trace-plots below, the mixing for Dikin is much better than hit-and-run
given the same sampling parameters.

<<example7, eval = TRUE, fig.width=5, fig.height=3>>=
set.seed(314)
N <- 50
A <- matrix(sample(c(0,3), N, replace = T), nrow = 1)
A <- rbind(A, matrix(sample(c(0,3), N, replace = T), nrow = 1))
b <- c(0.7, 0.3)

answer_hitandrun <- walkr(A = A, b = b, points = 500, method = "hit-and-run", 
                 thin = 10, burn = 0, chains = 1)
answer_dikin <- walkr(A = A, b = b, points = 500, method = "dikin", 
                 thin = 10, burn = 0, chains = 1)

plot(y = answer_hitandrun[50,], x = 1:500, 
     xlab = "random walk", ylab = "value", type = 'l', 
     main = "Hit-and-run Mixing",
     ylim = c(0, 0.2))
plot(y = answer_dikin[50,], x = 1:500, 
     xlab = "random walk", ylab = "value", type = 'l',
     main = "Dikin Mixing", 
     ylim = c(0, 0.2))
@

\section{Conclusion}

\pkg{walkr} uses MCMC random walks to sample $x$ from the intersection of two spaces. 
The first space is the complete solution space to the underdetermined matrix equation $Ax=b$, 
where $A$ is a $M \times N$ matrix, with $M < N$.  The second space is the $N$-Simplex, 
described by equation $x_1+x_2+...+x_N=1$ and inequalities
$x_i \geq 0$. This intersection is a convex polytope. 

In order to sample from this convex polytope, we perform an affine transformation which transforms
from $x$-space to $\alpha$-space, in which the $\alpha$'s are coefficients of basis vectors. Through
this transformation, we are able to re-express our sampling space as a generic $Ax \leq b$
convex polytope, which allows us to perform our sampling algorithms. Once the sampling is completed,
\pkg{walkr} maps the points back to the original coordinate system and returns them to the user.
This is all done internally by \pkg{walkr}, as the user only need to specifcy the original 
$A$ and $b$ in $Ax=b$. 

\pkg{walkr} implements two MCMC sampling algorithm -- hit-and-run and Dikin walk. The package also 
provides MCMC convergence diagnostics of the random walk's mixing, as well as a link to the \pkg{
shinystan} package which enables visualization. Hit-and-run is a widely used MCMC algorithm 
that guarantees uniform convergence asymptotically and has a relatively low computation cost for
one step. However, as the dimension of our convex polytope increases, hit-and-run mixes increasingly
slower. Dikin walk is an alternative MCMC algorithm that samples nearly uniformly, favoring 
points away from the edges of the polytope. While Dikin walk is only nearly uniform, it exhibits
much stronger mixing in high dimensions than hit-and-run does. 

\section{Authors}

\address{Andy Yao\\
Mathematics and Physics\\
Williams College \\
Williamstown, MA, USA\\}
\email{andy.yao17@gmail.com}

\address{David Kane\\
Managing Director \\
Hutchin Hill Capital\\
101 Federal Street, Boston, USA\\}
\email{dave.kane@gmail.com}

\bibliography{walkr}

\end{article}
\end{document}