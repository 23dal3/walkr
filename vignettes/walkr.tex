\documentclass[a4paper]{report}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{RJournal}
\usepackage{amsmath,amssymb,array}
\usepackage{booktabs}
\usepackage{float}
\usepackage[parfill]{parskip}
\usepackage[round]{natbib}
\usepackage{subfigure}

%\VignetteEngine{knitr::knitr} 
%\VignetteIndexEntry{walkr}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
%% do not edit, for illustration only
\sectionhead{Contributed research article}
\volume{XX}
\volnumber{YY}
\year{20ZZ}
\month{AAAA}

\begin{article}

\title{walkr}

\author{by Andy Yao, David Kane}

\maketitle      

\abstract{
The \texttt{walkr} package samples points using random walks from the intersection of 
the $N$ simplex with $M$ hyperplanes. Mathematically, the sampling space is all vectors $x$ 
that satisfy $Ax=b$, $\sum x = 1$, and $x_i \geq 0$. The sampling algorithms implemented 
are hit-and-run and Dikin walk, both of which are MCMC (Monte-Carlo Markov Chain) random 
walks. \texttt{walkr} also provide tools to examine and visualize the convergence
properties of the random walks.

}

\section{Introduction} 

Consider all possible vectors $x$ that satisfy the underdetermined matrix equation $Ax=b$, such that 
every component of $x$ is $\geq 0$ and sum to $1$. How do we generate a diverse sample of such 
$x$'s? The \texttt{walkr} package uses MCMC (Monte-Carlo Markov Chain) random walks to generate 
such a sample.

\texttt{walkr} contains two MCMC random walks. Our first random walk is hit-and-run. 
Hit-and-run is a widely used MCMC sampling method that guarantees uniform sampling asympotically, 
but mixes slower and slower as the dimensions of $A$ increase. Our second random walk is the 
Dikin Walk. Dikin Walk generates a nearly uniform sample and exhibits much stronger mixing. 
\code{walkr} uses \code{Rcpp}(\cite{rcpp}) and \code{RcppEigen}(\cite{rcppeigen}) 
for the core matrix calculations which take up most of the run-time.

\texttt{walkr} also provides statistical diagnostics of the mixing and convergence
properties of a MCMC random walk. 

\section{Mathematical Background of Sampling Space}

In this section, we go through the mathematical background of the space from which 
we are sampling -- the intersection of the $N$-simplex and hyperplanes. The reader does not need to 
read this section in order to use our package or understand the sampling algorithms. 
However, this section should provide the reader a better sense of what the sample space 
is geometrically and mathematically. 

\textbf{1) } $\mathbf{Ax = b}$ represent a system of $M$ linear equations with $N$ variables ($M \ll N$). 
Hence, $A$ is a $M \times N$ matrix ($M$ variables and $N$ constraints), $x$ is 
a $N \times 1$ vector, and $b$ is a $M \times 1$ vector. Every row in $A$ represents a hyperplane
in $\mathbb{R}^N$.

\textbf{2) The $\mathbf{N}$-dimensional unit simplex ($\mathbf{N}$-Simplex)} is described by:

$$x_1 + x_2 + x_3 + ... + x_N = 1$$
$$x_i \geq 0$$

\subsection{Sampling space: simple 3D case}

Let's begin with the simplest case -- one linear constraint in 3 dimensional space.

$$x_1 + x_3 = 0.5$$

We can express this in terms of matrix equation $Ax=b$, where:

$$
A = 
\begin{bmatrix}
1 & 0 & 1 \\
\end{bmatrix},
\quad
b = 0.5,
\quad
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
$$ 

In addition, we require the solution space to be intersected with the $3$D simplex: 

$$\sum x_i = 1$$
$$x_i \geq 0$$

In the following graph, we draw the intersection of the two. 
The orange equilateral triangle represents the 3D simplex, and the blue rectangle represents 
the plane $w_1+w_3=0.5$. The intersection of the hyperplane (blue) with the simplex (orange) 
is the red line segment, which is our sampling space.

% The figure below should be replaced with one that is better drawn

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/3Dcase.png}
\caption[caption]{The sampling space is the line segment (red), which is the intersection
                  of the 3D simplex (orange) and a hyperplane (blue)}
\end{figure}

\subsection{Matrix Representation of Multiple Hyperplanes}

Every hyperplane is described by one linear equation. Thus, a system of linear equations is the intersection
of hyperplanes. In general, if we have $M$ linear equations and $N$ variables, then $Ax=b$ would look like:

$$
A_{M \times N} = 
\underbrace{
  \begin{bmatrix}
   & & & & & & \\
   & & & ... & & & \\
   & & & & & & \\
%    &  & . & . & . &  & \\
%    &  & . & . & . &  & \\
%    &  & . & . & . &  & \\
%    & & & & & & \\
  \end{bmatrix}
}_\text{N columns (variables)}
\Bigg\}\text{{M rows (constraints)}}
$$

Again, $b$ is a $M \times 1$ vector, and $x$ is a $N \times 1$ vector.

\subsection{$4$D space}

Just like how the $3$D simplex is a $2$D surface living in $3$D space, the $4$D simplex 
(i.e. $x_1+x_2+x_3+x_4=1$, $x_i \geq 0$) could be viewed as a $3$D object. Specifically,
the $4$D simplex is the following tetradhedron when viewed from $3$D space, with 
verticies $(1,0,0,0)$, $(0,1,0,0)$, $(0,0,1,0)$, and $(0,0,0,1)$.\\

% Figure needs to be replaced

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/tetra1.png}
\caption[caption]{4D Simplex Can Live in 3D Space}
\end{figure}

Now imagine the intersection of the $4$D simplex with one hyperplane in $4$D (1 equation,
or 1 row in $Ax=b$). For a specific $A$ and $b$, we demonstrate the intersection 
in the figure below. The resulting shape is a trapezoid in $4$D space. 

$$
A = 
\begin{bmatrix}
1 & 1 & 1 & 1 \\
22 & 2 & 2 & 37\\
\end{bmatrix},
\quad
b = 
\begin{bmatrix}
1\\
16
\end{bmatrix}
$$

\begin{figure}[H]
\centering
\includegraphics[width = 4in, height = 2.5in]{img/tetra2.png}
\caption[caption]{Intersection of a 4D Simplex and a Hyperplane}
\end{figure}

In higher dimensions, the same logic applies. 
Each row in $Ax=b$ is a hyperplane living in $\mathbb{R}^N$ (given $N$
variables). Thus, geometrically, our sampling space is: \textbf{the intersection of hyperplanes
with the $N$-simplex}. 

\subsection{From $Ax=b$ and the $N$-simplex to $Ax \leq b$}

Our sampling space is bounded (i.e. has finite volume in 
$\mathbb{R}^N$). More formally, our sampling space is known as a \textbf{convex-polytope} 
in $\mathbb{R}^N$. Convex-polytopes are commonly 
described in the literature by a generic $Ax \leq b$. Here, we present a simple linear transformation 
which transforms the intersection of $Ax=b$ and the $N$-simplex to the form
$Ax \leq b$. \\

First, note that the equality part of the simplex constraint ($\sum x = 1$) could be added as an extra row 
in $Ax = b$

$$
A =
  \begin{bmatrix}
   & & ...... & & & \\
   & & ......  & & & \\
   & & ...... & & & \\
   1 & 1 & ...... & 1 & 1 \\

  \end{bmatrix}, 
\quad 
b = 
  \begin{bmatrix}
  ... \\
  ... \\
  ... \\
  1 \\
  \end{bmatrix}
$$

Second, to find the complete solution to the new $Ax = b$ (i.e. the set of all possible $x$'s that satisfy
$Ax=b$), we must find the Null Space of $A$(all $x$'s that satisfy $Ax=0$), 
then add on any particular solution to $Ax = b$ (This procedure can be found in any 
Linear Algebra textbook). \\

Mathematically, if the original $A$ was $M \times N$, then after adding on the extra row from the simplex, 
the basis vectors, each with $N$ components, which span the Null Space of our new $A$ will be: 

$$
\Bigg\{
v_1, \quad v_2, \quad v_3, \quad ...... \quad , \quad v_{N-(M+1)}
\Bigg\}
$$

\noindent Using any particular solution, $v_{particular}$, the complete solution to the new $Ax=b$
will be 

$$
\Bigg\{
v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad | \quad \alpha_i \in \mathbb{R}
\Bigg\}
$$ \\ 

Lastly, we tag on the $x_i \geq 0$ constraints, and with some algebraic manipulations:

$$v_{particular} + \alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq
\begin{bmatrix}
0 \\
0 \\
... \\
... \\
... \\
0 \\
\end{bmatrix}
$$

$$
\alpha_1v_1 + \alpha_2v_2 + \alpha_3v_3 + ... + \alpha_{N-(M+1)}v_{N-(M+1)}
\quad \geq -v_{particular}
$$

$$
V\alpha \geq -v_{particular}, \quad \text{where:} \quad
V = 
\begin{bmatrix}
v_1 &
v_2 &
... &
v_{N-(M+1)} \\
\end{bmatrix}, 
\quad
\alpha = 
\begin{bmatrix}
\alpha_1 \\
\alpha_2 \\
... \\
\alpha_{N-(M+1)}
\end{bmatrix}
$$

\noindent And finally, we arrive at the form $Ax \leq b$.

$$
-V\alpha \leq v_{particular}
$$

We have performed a \textbf{transformation} from "$x$-space" (
coordinates described by $x_1, x_2, ...$) to "$\alpha$-space" (coordinates described 
by $\alpha_1, \alpha_2, ...$). The geometric object described is still the same convex
polytope. In fact, \texttt{walkr} internally performs this transformation, 
samples the $\alpha$'s, maps them back to "$x$-space, and then
returns the sampled points. \\

The user need not be concerned with this transformation affecting the uniformity 
or mixing properties of our MCMC sampling algorithms. This is because the transformation
above is an affine transformation, which preserves uniformity.
Simply put, sampling in either space is equivalent. \\

Having understood that the intersection of $Ax=b$ with the unit-simplex is a convex polytope, 
we are ready to dive into the core of \texttt{walkr}
-- MCMC random walks.

\section{Random Walk: How to pick starting points?}

% analytic center?

MCMC random walks need a starting point, $x_0$, in the interior of the convex polytope. \texttt{walkr}
generates such starting points using linear programming.
Specifically, the \texttt{lsei} function of \texttt{limSolve} finds $x$ which:

$$\text{minimizes} \quad |Cx-d|^2$$
$$\text{subject to} \quad Ax \leq b$$

Thus, we randomly generate $C$ and $d$ obtaining $x$ which satisfy $Ax \leq b$. We discovered that 
the $x$'s generated this way fall randomly on the boundaries of our convex polytope, due to 
the minimizing property of linear programming. Thus, we repeat this for say,
10 times, and then take an average of the $x$'s generated. This averaged point is $x_0$, our starting point.

\section{Random Walk: Hit-and-run}

The hit-and-run algorithm is as follows:

\begin{enumerate}
  
  \item{Set starting point $x_0$ as current point}
  \item{Randomly generate a direction $\vec{d}$. If we are in $N$ dimensions, then $d$ will 
        be a vector of $N$ components. Specifically, $d$ is a uniformly generated 
        unit vector on the $N$ dimensional unit-sphere}
  \item{Find the chord $S$ through $x_0$ along the directions $\vec{d}$ and $-\vec{d}$.  
        We find end points $s_1$ and $s_2$ of the chord by going through the rows 
        of $Ax_0 \leq b$ one by one, setting the inequality to equality (so we hit the surface). 
        Then, parametrize the chord along $x_0$ by $s_1 + t(s_2-s_1)$, where $t \in [0,1]$}
  \item{Pick a random point $x_1$ along the chord $S$ by generating $t$ from \texttt{Uniform[0,1]}}
  \item{Set $x_1$ as current point}
  \item{Repeat algorithm until number of desired points sampled}
        
\end{enumerate}

Here is a picture of the hit-and-run algorithm: \newpage

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_trapezoid1.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun2.png}
\label{fig:subfigure1}}
\quad
\subfigure[Step 3]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun3.png}
\label{fig:subfigure2}}
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun4.png}
\label{fig:subfigure3}}
\quad
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_hitandrun5.png}
\label{fig:subfigure4}}
%
\caption{The hit-and-run algorithm begins with an interior point $x_0$ (Step 1). A random direction
         is selected (Step 2), and the chord along that direction is calculated (Step 3).
         Then, we pick a random point along that chord and move there as our new point (Step 4).
         The algorithm is repeated to sample many points (Step 5)}
\label{fig:figure}
\end{figure}

The core implementation of the hit-and-run algorithm calls the \code{har} function from the 
\code{hit-and-run} package on CRAN (\cite{hitandrun}). \code{walkr} serves as a convenient wrapper
with MCMC diagnostics and multiple chains. 

%probably also want to discuss important facts about the algorithm here.

\section{Random Walk: Dikin Walk}

\subsection{Preliminary Definitions}

Recall, our sampling space is a convex polytope. We call this convex 
polytope $K$, which can be described in the form $Ax \leq b$. \\

For the definitions below, let $a_i$ represent a row in $A$, $x_i$, $b_i$ represent 
the $i^{th}$ element of $x$ and $b$. Also recall that $A$ is a $M \times N$ matrix. \\

\noindent \textbf{Log Barrier Function $\phi$:} 

$$\phi(x) = \sum {- \log(b_i - a_i^Tx)}$$

\noindent We can compute and simplify the Hessian of the Log Barrier:\\

\noindent \textbf{Hessian of Log Barrier $H_x$:} 

$$H_x = \nabla ^2 \phi(x) = ...... = A^T D^2 A \quad , \quad \text{where:}$$
$$D = diag(\frac{1}{b_i - a_i^Tx})$$

\textbf{Note:} $H_x$ is a $N \times N$ linear operator. $D$ is a $M \times M$ diagonal matrix.\\

\noindent \textbf{Definition - Dikin Ellipsoid $D_{x_0}^r$} \\

$D_{x_0}^r$, the Dikin Ellipsoid centered at $x_0$ with radius $r$ is defined as: 

\begin{center}
$D_{x_0}^r \quad = \quad $\{$y \quad | \quad (y-x)^T H_{x_0}(y-x) \leq r^2$\} 
\end{center}

The shape of the Dikin Ellipsoid with radius $r$ is a function of $A$, $b$, and its center $x_0$.
Thus, throughout the convex polytope $K$, the Dikin Ellipsoid changes shape depending 
on where the center $x_0$ is. 

\subsection{Algorithm \texttt{Dikin}}

\begin{enumerate}

  \item{Begin with a point $x_0 \in K$. This starting point must be in the polytope.}
  \item{Construct $D_{x_0}$, the Dikin Ellipsoid centered at $x_0$}
  \item{Pick a random point $y$ from $D_{x_0}$}
  \item{If $x_0 \notin D_{y}$, then reject $y$ (be careful, this condition is counter-intuitive)}
  \item{If $x_0 \in D_{y}$, then accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ 
        (the big picture is that the ratio of the determinants are equal
         to the ratio of volumes of the ellipsoids centered at $x_0$ and $y$. Thus, 
         the geometric argument would be that 
         this way the Dikin walk can avoid extreme corners of the region)}
  \item{repeat until obtained number of desired points}

\end{enumerate}
\newpage

\begin{figure}[ht]
\centering
\subfigure[Step 1]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin2.png}
\label{fig:subfigure1}}
\subfigure[Step 2]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin3.png}
\label{fig:subfigure2}}
\quad
\subfigure[Step 3 Case I]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin41.png}
\label{fig:subfigure3}}
\subfigure[Step 3 Case II]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin42.png}
\label{fig:subfigure4}}
\quad
\subfigure[Step 4]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin5.png}
\label{fig:subfigure5}}
\subfigure[Step 5]{%
\includegraphics[width = 1.95in, height = 1.5in]{img/pics/exp_dikin6.png}
\label{fig:subfigure6}}

%
\caption{The Dikin Walk begins by constructing the Dikin Ellipsoid at the starting point $x_0$ (Step 1). 
         An uniformly random point $y$ is generated in the Dikin Ellipsoid centered at $x_0$ (Step 2). 
         If point $x_0$ is not in the Dikin Ellipsoid centered at $y$, then reject $y$ (Step 3 Case I).
         If point $x_0$ is contained in the Dikin Ellipsoid centered at $y$, then 
         accept $y$ with probability $\min(1, \sqrt{\frac{det(H_{y})}{det(H_{x_0})}})$ (Step 3 Case II).
         Once we've successfully accepted $y$, we set $y$ as our new point, $x_1$ (Step 4). 
         Algorithm repeats (Step 5)}
         
\label{fig:figure}
\end{figure}

\subsection{How to pick a random point uniformly from a Dikin Ellipsoid?}

Let's say, we now have $D_{x}^r$, the Dikin Ellipsoid centered at $x$ with radius $r$. 

\begin{enumerate}

  \item{generate $\zeta$ from the $n$ dimensional Standard Gaussian (i.e. \texttt{zeta = rnorm(n,0,1)})}
  \item{normalize $\zeta$ to be on the $n$ dimensional ball with radius $r$, that is:}
    \subitem{$\zeta \quad = \quad <x_1,x_2,...,x_n> \quad \rightarrow \quad <\frac{rx_1}
            {\sqrt{x_1^2+x_2^2+...+x_n^2}}, 
            \frac{rx_2}{\sqrt{x_1^2+x_2^2+...+x_n^2}}, ...... , 
            \frac{rx_n}{\sqrt{x_1^2+x_2^2+...+x_n^2}}>$}
  \item{Solve for $d$ in the matrix equation $H_x d = A^TD\zeta$ (note, as long as $x_0$ is not on the boundary
        of our polytope $K$, $H_x$ will be non-singular, thus, $d$ will always be unique)}
  \item{$y = x_0 + d$ is our randomly sampled point from $D_x^r$}

\end{enumerate}

\subsection{Important Theorem}

In algorithm \texttt{Dikin}, what if the point $y$ we accept is outside 
of our polytope $K$? Luckily, there is no need to worry about that because of the following theorem: \\

\noindent \textbf{Theorem} -- If $x_0 \in K$, then $D_{x_0}^1 \subseteq K$. That is, 
if our starting point $x_0$ is in our polytope $K$, then the Dikin Ellipsoid centered 
at $x_0$ with radius $1$ will always be contained in $K$. \\

Thus, if we set $r=1$ (or $r \leq 1$), then our algorithm will guarantee to sample points only 
within the polytope $K$.

\section{Using \texttt{walkr} to sample points}

%% this is helpful : http://yihui.name/knitr/options/


\texttt{walkr} has one main function \code{walkr} which sample points. \\

First, the user must specify the $A$ and $b$ in the $Ax=b$ hyperplanes equation. For example,
for a 5 dimensional case:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(walkr)}
\hlstd{A} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{ncol} \hlstd{=} \hlnum{5}\hlstd{)}
\hlstd{b} \hlkwb{<-} \hlnum{0.5}
\hlstd{A}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    1    0    1
\end{verbatim}
\begin{alltt}
\hlstd{b}
\end{alltt}
\begin{verbatim}
## [1] 0.5
\end{verbatim}
\end{kframe}
\end{knitrout}

The \texttt{walkr} function takes care of the intersection with the simplex for the user. However,
if the user just want to sample from the simplex with no hyperplanes, she can simply specify the
simplex equation, as \texttt{walkr} will remove linearly dependent rows internally.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{A} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{ncol} \hlstd{=} \hlnum{5}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

After we have \code{A} and \code{B}, we are now ready for sampling. \texttt{walkr} has the following
parameters:

\begin{itemize}

  \item{ \code{A} is the right hand side of the matrix equation $Ax=b$}
  \item{ \code{b} is the left hand side of the matrix equation $Ax=b$}
  \item{ \code{method} is the method of sampling -- can be either \code{"hit-and-"run"} or 
         \code{"dikin"}}
  \item{ \code{thin} is the thinning parameter. Every \code{thin}-th point is stored into the final sample}
  \item{ \code{burn} is the burning parameter. The first \code{burn} points are deleted from the final sample}
  \item{ \code{chains} is the number of chains we want to sample. Every chain is an element 
        of the list which \code{walkr} eventually returns}

\end{itemize}

\code{walkr} has two components to telling the user whether few chains of the MCMC walk has mixed
"well enough". First is an interal warning message within walkr before the results are returned. 

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{## sampling from the 3D simplex}
\hlkwd{library}\hlstd{(walkr)}
\hlstd{A} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{1}\hlstd{,} \hlkwc{ncol} \hlstd{=} \hlnum{3}\hlstd{)}
\hlstd{b} \hlkwb{<-} \hlnum{1}
\hlstd{answer} \hlkwb{<-} \hlkwd{walkr}\hlstd{(}\hlkwc{A} \hlstd{= A,} \hlkwc{b} \hlstd{= b,} \hlkwc{points} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{method} \hlstd{=} \hlstr{"hit-and-run"}\hlstd{,}
                \hlkwc{thin} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{burn} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{chains} \hlstd{=} \hlnum{5}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\color{warningcolor}{\#\# Warning in walkr(A = A, b = b, points = 10, method = "{}hit-and-run"{}, thin = 1, : there are parameters with rhat > 1.1, you may want to run your chains for longer}}\begin{alltt}
\hlstd{answer}
\end{alltt}
\begin{verbatim}
##            [,1]       [,2]        [,3]       [,4]       [,5]       [,6]
## [1,] 0.23959806 0.07932791 0.251327998 0.22643520 0.27408170 0.26710634
## [2,] 0.66378904 0.88223408 0.745745780 0.73837994 0.67981092 0.71713803
## [3,] 0.09661291 0.03843801 0.002926221 0.03518486 0.04610738 0.01575563
##            [,7]      [,8]       [,9]      [,10]
## [1,] 0.08973057 0.1338480 0.08659668 0.03719618
## [2,] 0.23544689 0.1034884 0.44895932 0.57834991
## [3,] 0.67482254 0.7626636 0.46444399 0.38445391
\end{verbatim}
\end{kframe}
\end{knitrout}

As we see from the code chunk above, \code{walkr} returns a warning message that the $\hat{R}$ are not satisfying, since we only have 10 points. However, if we run the chain for long (or increasing thinning), 
then the statistical tests pass.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(walkr)}
\hlstd{answer} \hlkwb{<-} \hlkwd{walkr}\hlstd{(}\hlkwc{A} \hlstd{= A,} \hlkwc{b} \hlstd{= b,} \hlkwc{points} \hlstd{=} \hlnum{1000}\hlstd{,} \hlkwc{method} \hlstd{=} \hlstr{"hit-and-run"}\hlstd{,}
                \hlkwc{thin} \hlstd{=} \hlnum{1}\hlstd{,} \hlkwc{burn} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{chains} \hlstd{=} \hlnum{5}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\section{Dikin versus Hitandrun}
%reference here:{http://www.mit.edu/~har/Dikin.pdf}
\begin{tabular}{ |p{3cm}|p{5cm}|p{5cm}|}
  \hline
   &  \texttt{hit-and-run} & \texttt{Dikin Walk} \\ \hline
  Uniform Sampling & Yes, needs $O(N^3)$ points, where $N$ is the dimension of the polytope & 
  No, concentrates in the    interior\\ \hline
  Mixing & $O(\frac{N^2R^2}{r^2})$ *, slows down substantially as dimension of polytope increases and 
  polytope becomes "skinnier" & $O(MN)$, where $A$ is $M \times N$; much stronger mixing. \\ \hline
  Cost of One Step & $O(MN)$ & $O(MN^2)$, in practice, one step of Dikin is much more 
  costly than hit-and-run\\ \hline
  Rejection Sampling & No & Yes (see probability formula and $x \notin D_y$), but rejection rate 
  not high\\ \hline

\end{tabular} \\

*$R$ is the radius of the smallest ball that contains the polytope $K$. $r$ is the radius of the 
largest ball that is contained within the polytope $K$. Thus, $\frac{R}{r}$ increases 
as the polytope is "skinnier"(\cite{kannan}). \\

\textbf{NEED TO REPLACE THE FOLLOWING TWO PLOTS WITH REPRODUCIBLE CODE CHUNKS (3D HIST IF POSSIBLE)}

\begin{figure}[H]
\centering
\begin{minipage}{.4\textwidth}
\includegraphics[width = 1.95in, height = 1.5in]{img/dikintrap.png}
\caption[caption]{Dikin concentrates at the center}
\end{minipage}%
\centering
\begin{minipage}{.4\textwidth}
\includegraphics[width = 1.95in, height = 1.5in]{img/hartrap.png}
\caption[caption]{Hit-and-run samples the space thoroughly}
\end{minipage}
\end{figure}

As we can in the two trace-plots below, the mixing for Dikin is much better than hit-and-run
given the same set of parameters

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hlstd{(}\hlnum{314}\hlstd{)}
\hlstd{N} \hlkwb{<-} \hlnum{50}
\hlstd{A} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{sample}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{3}\hlstd{), N,} \hlkwc{replace} \hlstd{= T),} \hlkwc{nrow} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{A} \hlkwb{<-} \hlkwd{rbind}\hlstd{(A,} \hlkwd{matrix}\hlstd{(}\hlkwd{sample}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{3}\hlstd{), N,} \hlkwc{replace} \hlstd{= T),} \hlkwc{nrow} \hlstd{=} \hlnum{1}\hlstd{))}
\hlstd{b} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{0.7}\hlstd{,} \hlnum{0.3}\hlstd{)}

\hlstd{answer_hitandrun} \hlkwb{<-} \hlkwd{walkr}\hlstd{(}\hlkwc{A} \hlstd{= A,} \hlkwc{b} \hlstd{= b,} \hlkwc{points} \hlstd{=} \hlnum{500}\hlstd{,} \hlkwc{method} \hlstd{=} \hlstr{"hit-and-run"}\hlstd{,}
                 \hlkwc{thin} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{burn} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{chains} \hlstd{=} \hlnum{1}\hlstd{)}
\hlstd{answer_dikin} \hlkwb{<-} \hlkwd{walkr}\hlstd{(}\hlkwc{A} \hlstd{= A,} \hlkwc{b} \hlstd{= b,} \hlkwc{points} \hlstd{=} \hlnum{500}\hlstd{,} \hlkwc{method} \hlstd{=} \hlstr{"dikin"}\hlstd{,}
                 \hlkwc{thin} \hlstd{=} \hlnum{10}\hlstd{,} \hlkwc{burn} \hlstd{=} \hlnum{0}\hlstd{,} \hlkwc{chains} \hlstd{=} \hlnum{1}\hlstd{)}

\hlkwd{plot}\hlstd{(}\hlkwc{y} \hlstd{= answer_hitandrun[}\hlnum{50}\hlstd{,],} \hlkwc{x} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{500}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"random walk"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"value"}\hlstd{,} \hlkwc{type} \hlstd{=} \hlstr{'l'}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Hit-and-run Mixing"}\hlstd{,}
     \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.2}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/example7-1} 
\begin{kframe}\begin{alltt}
\hlkwd{plot}\hlstd{(}\hlkwc{y} \hlstd{= answer_dikin[}\hlnum{50}\hlstd{,],} \hlkwc{x} \hlstd{=} \hlnum{1}\hlopt{:}\hlnum{500}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"random walk"}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"value"}\hlstd{,} \hlkwc{type} \hlstd{=} \hlstr{'l'}\hlstd{,}
     \hlkwc{main} \hlstd{=} \hlstr{"Dikin Mixing"}\hlstd{,}
     \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0.2}\hlstd{))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/example7-2} 

\end{knitrout}


\section{Conclusion}

\code{walkr} samples $x$ that satisfy the underdetermined matrix equation $Ax=b$, every component
of $x$ is non-negative, and every component sum to $1$. \code{walkr} internally performs the affine
transformation which transforms $Ax=b$ and the simplex constraints into the $Ax \leq b$ form. Then, 
\code{walkr} samples points under this transform, takes the inverse transform, and finally
returns the $x$'s sampled.

The two methods that the user may specify is \code{"dikin"} and \code{"hit-and-run"}. Hit-and-run
is a widely used sampling method that converges to the uniform sampling asymptotically, but 
fails to mix well in higher dimensions. Dikin generates a nearly uniform sample, but mixes stronger than
hit-and-run, especially in higher dimensions. In addition, \code{walkr} will warn the user if the 
mixing is not strong enough according to the Gelman-Rubin Diagnostics.

\section{Authors}

\address{Andy Yao\\
Mathematics and Physics\\
Williams College \\
Williamstown, MA, USA\\}
\email{ay3@williams.edu}

\address{David Kane\\
Managing Director \\
Hutchin Hill Capital\\
101 Federal Street, Boston, USA\\}
\email{dave.kane@gmail.com}

\bibliography{walkr}

\end{article}
\end{document}
